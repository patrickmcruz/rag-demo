# FAQ - Perguntas Frequentes

## üìö √çndice

- [Modelos Utilizados](#modelos-utilizados)
- [Sanitiza√ß√£o e Tokeniza√ß√£o](#sanitiza√ß√£o-e-tokeniza√ß√£o)
- [Embeddings e Performance](#embeddings-e-performance)
- [Troubleshooting](#troubleshooting)

---

## Modelos Utilizados

### ‚ùì Qual √© o modelo principal (LLM) usado no projeto?

O modelo principal √© o **Llama 3**, executado localmente via **Ollama**.

#### Especifica√ß√µes:

```yaml
Modelo: llama3
Provider: Ollama (local)
Custo: Gratuito (100% local)
Privacidade: Total (sem envio de dados)
Configura√ß√£o: .env ‚Üí OLLAMA_MODEL=llama3
```

#### Por que Llama 3?

**Vantagens:**
- ‚úÖ **Open source** e gratuito
- ‚úÖ **Execu√ß√£o local** - privacidade total
- ‚úÖ **√ìtima qualidade** - compar√°vel a GPT-3.5
- ‚úÖ **Multil√≠ngue** - suporta portugu√™s bem
- ‚úÖ **Flex√≠vel** - v√°rios tamanhos (8B, 70B)
- ‚úÖ **Sem limites de uso** ou custos de API

**Desvantagens:**
- ‚ö†Ô∏è Requer hardware local (GPU recomendada)
- ‚ö†Ô∏è Mais lento que APIs cloud
- ‚ö†Ô∏è Menor context window que GPT-4

#### Modelos alternativos suportados:

O projeto suporta qualquer modelo do Ollama. Para trocar:

```bash
# 1. Baixar modelo alternativo
ollama pull phi3          # R√°pido, 3.8GB
ollama pull mistral       # Balanceado, 4.1GB
ollama pull codellama     # Especializado em c√≥digo, 3.8GB
ollama pull gemma2        # Google, 5.4GB

# 2. Configurar no .env
OLLAMA_MODEL=phi3
```

**Compara√ß√£o de modelos:**

| Modelo | Tamanho | RAM | Qualidade | Velocidade | Uso Ideal |
|--------|---------|-----|-----------|------------|-----------|
| **llama3:8b** | 4.7GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Geral, balanceado |
| phi3 | 3.8GB | 6GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | R√°pido, eficiente |
| mistral | 4.1GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Bom para instru√ß√µes |
| codellama | 3.8GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | C√≥digo, t√©cnico |
| llama3:70b | 39GB | 64GB+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | M√°xima qualidade |

#### Como usar modelos cloud (produ√ß√£o):

Para ambientes de produ√ß√£o, voc√™ pode integrar APIs:

```python
# OpenAI
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4", temperature=0.0)

# Anthropic
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-sonnet")

# Google Vertex AI
from langchain_google_vertexai import ChatVertexAI
llm = ChatVertexAI(model="gemini-pro")
```

---

### ‚ùì Qual modelo de embedding √© usado? √â otimizado para portugu√™s?

O modelo de embedding padr√£o √© o **all-MiniLM-L6-v2**, que √© otimizado para **ingl√™s**.

#### Especifica√ß√µes:

```yaml
Modelo: sentence-transformers/all-MiniLM-L6-v2
Base: BERT (Microsoft)
Idioma: Ingl√™s (EN)
Dimens√µes: 384
Tamanho: ~80MB
Max tokens: 256
Performance: R√°pido e eficiente
```

#### ‚ö†Ô∏è Limita√ß√£o importante:

**N√£o √© otimizado para portugu√™s!** O modelo foi treinado principalmente em ingl√™s, o que pode impactar:
- Qualidade dos embeddings para textos em PT-BR
- Similaridade sem√¢ntica entre documentos
- Precis√£o do retrieval

#### ‚úÖ Modelos recomendados para portugu√™s:

**1. NeuralMind BERT (melhor para PT-BR):**
```python
EMBEDDING_MODEL=neuralmind/bert-base-portuguese-cased

# Caracter√≠sticas:
# - Treinado especificamente em portugu√™s brasileiro
# - 768 dimens√µes (maior precis√£o)
# - ~410MB
# - Melhor desempenho em textos PT-BR
```

**2. Multilingual MiniLM (bom compromisso):**
```python
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Caracter√≠sticas:
# - Suporta 50+ idiomas incluindo portugu√™s
# - 384 dimens√µes
# - ~420MB
# - Bom para projetos multil√≠ngues
```

**3. mBERT (multil√≠ngue):**
```python
EMBEDDING_MODEL=bert-base-multilingual-cased

# Caracter√≠sticas:
# - Suporta 104 idiomas
# - 768 dimens√µes
# - ~680MB
# - Google, bem estabelecido
```

#### Compara√ß√£o detalhada:

| Modelo | Idioma | Dimens√µes | Tamanho | Qualidade PT-BR | Velocidade |
|--------|--------|-----------|---------|-----------------|------------|
| **all-MiniLM-L6-v2** | EN | 384 | 80MB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **neuralmind/bert-base-portuguese-cased** | PT-BR | 768 | 410MB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| paraphrase-multilingual-MiniLM-L12-v2 | Multi | 384 | 420MB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| all-mpnet-base-v2 | EN | 768 | 420MB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| mBERT | Multi | 768 | 680MB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |

#### Como trocar o modelo de embedding:

**1. Atualizar .env:**
```bash
EMBEDDING_MODEL=neuralmind/bert-base-portuguese-cased
```

**2. Reindexar documentos:**
```bash
# Deletar vectorstore antigo
rm -rf vectorstore/

# Reindexar com novo modelo
python main.py ingest
```

**3. Atualizar c√≥digo (se necess√°rio):**
```python
# src/ingest.py e src/chain.py j√° suportam
# Basta mudar a vari√°vel de ambiente
ingestor = DocumentIngestor(
    embedding_model="neuralmind/bert-base-portuguese-cased"
)
```

#### üí° Recomenda√ß√£o para documentos em portugu√™s:

Para **melhor qualidade** em portugu√™s brasileiro:

```bash
# .env
OLLAMA_MODEL=llama3                                    # LLM: Llama 3 (suporta PT-BR)
EMBEDDING_MODEL=neuralmind/bert-base-portuguese-cased  # Embeddings: otimizado PT-BR
```

**Benef√≠cios esperados:**
- ‚úÖ Melhor compreens√£o sem√¢ntica em portugu√™s
- ‚úÖ Retrieval mais preciso
- ‚úÖ Respostas mais relevantes
- ‚úÖ Menos "perdas" na tradu√ß√£o de conceitos

**Trade-off:**
- ‚ö†Ô∏è Modelo maior (410MB vs 80MB)
- ‚ö†Ô∏è ~2-3x mais lento na indexa√ß√£o
- ‚ö†Ô∏è Mais uso de mem√≥ria RAM

#### Testando diferentes modelos:

```python
# Script de compara√ß√£o
from src.ingest import DocumentIngestor
import time

models = [
    "all-MiniLM-L6-v2",
    "neuralmind/bert-base-portuguese-cased",
    "paraphrase-multilingual-MiniLM-L12-v2"
]

for model in models:
    print(f"\nTestando: {model}")
    start = time.time()
    
    ingestor = DocumentIngestor(embedding_model=model)
    # ... indexar documentos ...
    
    print(f"Tempo: {time.time() - start:.2f}s")
```

---

## Sanitiza√ß√£o e Tokeniza√ß√£o

### ‚ùì Este projeto faz sanitiza√ß√£o dos dados antes de gerar os embeddings?

**N√£o**, atualmente o projeto **n√£o realiza sanitiza√ß√£o expl√≠cita** dos dados antes de gerar embeddings. O fluxo √© direto:

```
Documento ‚Üí Loader ‚Üí Split ‚Üí Embedding ‚Üí Chroma
```

As √∫nicas "limpezas" que acontecem s√£o:
- **`.strip()`** nas queries do usu√°rio (para remover espa√ßos em branco)
- **Nada nos documentos originais** - o texto √© usado como est√°

#### O que os loaders fazem:

1. **TextLoader**: L√™ arquivo texto bruto sem processamento
2. **PyPDFLoader**: Extrai texto do PDF (pode incluir caracteres especiais, quebras de linha estranhas)
3. **UnstructuredMarkdownLoader**: Processa Markdown b√°sico

#### ‚ö†Ô∏è Problemas potenciais sem sanitiza√ß√£o:

- M√∫ltiplos espa√ßos em branco consecutivos
- Caracteres especiais/Unicode mal formados
- Headers/footers repetitivos de PDFs
- Formata√ß√£o inconsistente entre documentos
- Metadados ou "lixo" de documentos digitalizados

#### ‚úÖ Melhorias recomendadas:

Adicionar uma fun√ß√£o de sanitiza√ß√£o no pipeline de ingest√£o:

```python
import re
import unicodedata

def sanitize_text(text: str) -> str:
    """Sanitize text before embedding."""
    # Remove m√∫ltiplos espa√ßos
    text = re.sub(r'\s+', ' ', text)
    
    # Remove caracteres de controle
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    
    # Normaliza Unicode (NFKC = compatibilidade)
    text = unicodedata.normalize('NFKC', text)
    
    # Remove linhas vazias m√∫ltiplas
    text = re.sub(r'\n\s*\n', '\n\n', text)
    
    return text.strip()
```

---

### ‚ùì Que tipo de tokeniza√ß√£o √© usada?

A tokeniza√ß√£o acontece **dentro do modelo de embeddings**, n√£o explicitamente no c√≥digo do projeto.

#### Modelo e Tokenizer:

```
Modelo: sentence-transformers/all-MiniLM-L6-v2
Tokenizer: WordPiece (baseado em BERT)
Vocabul√°rio: ~30.000 tokens
Limite: 256 tokens por sequ√™ncia
```

#### Como funciona atualmente:

1. **RecursiveCharacterTextSplitter** divide por **caracteres**:
   ```python
   separators=["\n\n", "\n", " ", ""]  # N√£o √© tokeniza√ß√£o!
   chunk_size=500  # 500 caracteres, n√£o tokens
   chunk_overlap=50  # 50 caracteres de overlap
   ```

2. **HuggingFaceEmbeddings** tokeniza internamente:
   - Usa o tokenizer WordPiece do modelo
   - Trunca automaticamente para 256 tokens se necess√°rio
   - Adiciona tokens especiais: `[CLS]` (in√≠cio) e `[SEP]` (fim)

#### ‚ö†Ô∏è Problema identificado:

O split √© feito por **caracteres** (500), mas o limite do modelo √© **256 tokens**. 

- Um chunk de 500 caracteres pode ter ~100-150 tokens (depende do idioma)
- N√£o h√° garantia de que todos os chunks caibam no limite do modelo
- Chunks muito longos s√£o truncados silenciosamente

#### ‚úÖ Melhorias recomendadas:

**1. Usar TokenTextSplitter** (divide por tokens, n√£o caracteres):

```python
from langchain_text_splitters import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=200,      # tokens, n√£o caracteres
    chunk_overlap=20,    # overlap em tokens
    encoding_name="cl100k_base"  # ou use o tokenizer do modelo
)
```

**2. Validar tamanho dos chunks**:

```python
def validate_chunk(chunk: str, max_tokens: int = 256) -> bool:
    """Validate chunk size in tokens."""
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained(
        "sentence-transformers/all-MiniLM-L6-v2"
    )
    tokens = tokenizer.encode(chunk)
    
    if len(tokens) > max_tokens:
        logger.warning(
            f"Chunk exceeds {max_tokens} tokens: {len(tokens)} tokens"
        )
        return False
    
    return True
```

**3. Splitting baseado no tokenizer do modelo**:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer

# Carregar tokenizer do modelo de embeddings
tokenizer = AutoTokenizer.from_pretrained(
    "sentence-transformers/all-MiniLM-L6-v2"
)

# Usar fun√ß√£o de contagem de tokens
def token_length(text: str) -> int:
    return len(tokenizer.encode(text))

# Configurar splitter com fun√ß√£o de tokens
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,          # tamanho em tokens
    chunk_overlap=20,        # overlap em tokens
    length_function=token_length,  # conta tokens, n√£o caracteres
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

---

### ‚ùì Como melhorar a qualidade dos embeddings?

#### Estrat√©gias recomendadas:

**1. Pr√©-processamento consistente:**
```python
def preprocess_for_embedding(text: str) -> str:
    """Preprocess text for better embeddings."""
    # Sanitiza√ß√£o b√°sica
    text = sanitize_text(text)
    
    # Remover URLs
    text = re.sub(r'http[s]?://\S+', '', text)
    
    # Remover emails
    text = re.sub(r'\S+@\S+', '', text)
    
    # Normalizar n√∫meros (opcional)
    # text = re.sub(r'\d+', '<NUM>', text)
    
    return text
```

**2. Chunks sem√¢nticos (n√£o apenas por tamanho):**
```python
# Dividir por par√°grafos/se√ß√µes primeiro
text_splitter = RecursiveCharacterTextSplitter(
    separators=[
        "\n## ",      # Headers Markdown
        "\n### ",
        "\n\n",       # Par√°grafos
        "\n",         # Linhas
        ". ",         # Senten√ßas
        " ",          # Palavras
        ""
    ],
    chunk_size=200,
    chunk_overlap=20,
    length_function=token_length
)
```

**3. Adicionar metadata relevante:**
```python
# Preservar contexto nos metadados
for chunk in chunks:
    chunk.metadata.update({
        "source": doc.metadata["source"],
        "page": doc.metadata.get("page", 0),
        "section": extract_section_name(chunk.page_content),
        "chunk_index": i,
    })
```

**4. Modelos de embedding alternativos:**
```python
# Para textos em portugu√™s, considere:
EMBEDDING_MODEL = "neuralmind/bert-base-portuguese-cased"
# ou
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
```

---

## Embeddings e Performance

### ‚ùì Por que usar all-MiniLM-L6-v2?

**Vantagens:**
- ‚úÖ R√°pido (apenas 6 layers)
- ‚úÖ Leve (~80MB)
- ‚úÖ Bom desempenho geral
- ‚úÖ Funciona offline
- ‚úÖ Sem custos de API

**Desvantagens:**
- ‚ö†Ô∏è Otimizado para ingl√™s
- ‚ö†Ô∏è Limite de 256 tokens
- ‚ö†Ô∏è Menos preciso que modelos maiores

**Alternativas:**

| Modelo | Tamanho | Idioma | Dimens√µes | Uso |
|--------|---------|--------|-----------|-----|
| `all-MiniLM-L6-v2` | 80MB | EN | 384 | Geral, r√°pido |
| `paraphrase-multilingual-MiniLM-L12-v2` | 420MB | Multi | 384 | Multil√≠ngue |
| `all-mpnet-base-v2` | 420MB | EN | 768 | Melhor qualidade |
| `neuralmind/bert-base-portuguese-cased` | 410MB | PT-BR | 768 | Portugu√™s |

---

### ‚ùì Como otimizar a performance do sistema?

**1. Cache de embeddings:**
```python
# Evitar re-embeddings de documentos j√° processados
import hashlib

def get_doc_hash(doc: Document) -> str:
    return hashlib.md5(doc.page_content.encode()).hexdigest()

# Verificar se embedding j√° existe antes de processar
```

**2. Batch processing:**
```python
# Processar m√∫ltiplos documentos de uma vez
vectorstore = Chroma.from_documents(
    documents=all_splits,
    embedding=embedding,
    persist_directory=persist_dir,
    batch_size=100  # Ajustar conforme mem√≥ria
)
```

**3. Configurar Chroma adequadamente:**
```python
# Usar configura√ß√£o otimizada
from chromadb.config import Settings

chroma_settings = Settings(
    anonymized_telemetry=False,
    allow_reset=True,
    is_persistent=True
)
```

---

## Troubleshooting

### ‚ùì Erro: "Token indices sequence length is longer than the maximum"

**Causa:** Chunks maiores que 256 tokens.

**Solu√ß√£o:** Reduzir `chunk_size` ou usar token-based splitting:

```python
# Op√ß√£o 1: Reduzir chunk_size
DocumentIngestor(chunk_size=300, chunk_overlap=30)

# Op√ß√£o 2: Usar TokenTextSplitter
from langchain_text_splitters import TokenTextSplitter
splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=20)
```

---

### ‚ùì PDFs com texto mal formatado

**Causa:** PDFs escaneados ou com formata√ß√£o complexa.

**Solu√ß√µes:**

```python
# 1. Usar OCR para PDFs escaneados
from langchain_community.document_loaders import PyMuPDFLoader
loader = PyMuPDFLoader("document.pdf")  # Melhor extra√ß√£o

# 2. Limpar texto extra√≠do
def clean_pdf_text(text: str) -> str:
    # Remove h√≠fens de quebra de linha
    text = re.sub(r'-\n', '', text)
    
    # Remove quebras de linha no meio de palavras
    text = re.sub(r'(\w)\n(\w)', r'\1 \2', text)
    
    # Normaliza espa√ßos
    text = re.sub(r' +', ' ', text)
    
    return text
```

---

### ‚ùì Respostas gen√©ricas ou imprecisas

**Poss√≠veis causas e solu√ß√µes:**

**1. Poucos documentos recuperados:**
```python
# Aumentar top_k
chain = create_rag_chain(top_k=5)  # default √© 3
```

**2. Chunks muito grandes ou pequenos:**
```python
# Ajustar tamanho ideal (200-300 tokens)
DocumentIngestor(chunk_size=400, chunk_overlap=50)
```

**3. Prompt inadequado:**
```python
# Melhorar prompt no chain.py
template = """Voc√™ √© um especialista em [DOM√çNIO].
Analise cuidadosamente o contexto fornecido.

Contexto:
{context}

Pergunta: {question}

Instru√ß√µes:
1. Responda APENAS com informa√ß√µes do contexto
2. Cite trechos relevantes
3. Se n√£o souber, diga claramente

Resposta detalhada:"""
```

**4. Temperatura muito alta:**
```python
# Reduzir temperatura para respostas mais determin√≠sticas
chain = create_rag_chain(temperature=0.0)  # mais factual
```

---

### ‚ùì Ollama n√£o conecta

**Verifica√ß√µes:**

```powershell
# 1. Verificar se Ollama est√° rodando
ollama list

# 2. Verificar se modelo existe
ollama pull llama3

# 3. Testar conex√£o
curl http://localhost:11434/api/tags

# 4. Verificar vari√°vel de ambiente
echo $env:OLLAMA_BASE_URL
```

**Configurar .env:**
```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
```

---

## üìö Refer√™ncias

- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [Sentence Transformers Documentation](https://www.sbert.net/)
- [Chroma Vector Database](https://docs.trychroma.com/)
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

**√öltima atualiza√ß√£o:** 2025  
**Contribui√ß√µes:** Envie PRs ou abra issues com mais perguntas!
