# RAG Demo - Sistema RAG Profissional com LangChain

<div align="center">

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1.0-green.svg)
![License](https://img.shields.io/badge/License-GNU%20GPL-blue)

Sistema de Retrieval-Augmented Generation (RAG) profissional usando LangChain, Chroma e Ollama.

</div>

## ğŸ“‹ Ãndice

- [Sobre](#-sobre)
- [Arquitetura](#-arquitetura)
- [Funcionalidades](#-funcionalidades)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Uso](#-uso)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Testes](#-testes)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [PrÃ³ximos Passos](#-prÃ³ximos-passos)
- [FAQ](#-faq)

## ğŸ¯ Sobre

Este projeto implementa um sistema RAG (Retrieval-Augmented Generation) completo e profissional, seguindo as melhores prÃ¡ticas de engenharia de software:

- **Modular e testÃ¡vel**: CÃ³digo organizado com separaÃ§Ã£o clara de responsabilidades
- **Logging e validaÃ§Ãµes**: Tratamento de erros robusto e logs informativos
- **Suporte multi-formato**: PDF, TXT, Markdown
- **ConfigurÃ¡vel**: VariÃ¡veis de ambiente para todas as configuraÃ§Ãµes
- **Documentado**: Docstrings completas e type hints
- **Preparado para produÃ§Ã£o**: Estrutura escalÃ¡vel e manutenÃ­vel

## ğŸ—ï¸ Arquitetura

```
[UsuÃ¡rio]
   â†“ (pergunta)
[Query Interface (query.py)]
   â†“
1. Embedding da pergunta
   â†“
2. Retrieval no Chroma (top-k documentos)
   â†“
3. Montagem do prompt com contexto
   â†“
4. Chamada ao LLM (Ollama)
   â†“
5. Resposta + metadados (fontes, tempo)
   â†“
[Resposta Estruturada]
```

### Componentes principais:

- **ingest.py**: Carrega, processa e indexa documentos
- **chain.py**: Define e configura a chain RAG
- **query.py**: Interface de alto nÃ­vel para consultas

## âœ¨ Funcionalidades

### Ingestion Pipeline
- Carregamento de mÃºltiplos formatos (TXT, PDF, MD)
- Splitting inteligente de documentos
- Embeddings com HuggingFace (sentence-transformers)
- IndexaÃ§Ã£o persistente com Chroma
- Logging detalhado de todo o processo

### RAG Chain
- ConfiguraÃ§Ã£o flexÃ­vel (temperatura, top-k, etc.)
- Suporte para mÃºltiplos modelos Ollama
- Prompts otimizados (PT/EN)
- ValidaÃ§Ãµes e tratamento de erros

### Query Interface
- CLI interativo
- Respostas estruturadas com metadados
- Rastreamento de fontes
- MÃ©tricas de performance
- HistÃ³rico de consultas

## ğŸ”§ PrÃ©-requisitos

### Sistema
- **Python 3.9+** (testado com Python 3.12)
- **4GB+ RAM** (para embeddings e modelos)
- **~2GB de espaÃ§o em disco** (para modelos e Ã­ndices)
- **Windows 10/11, Linux ou macOS**

### Software NecessÃ¡rio

#### 1. Python e pip
```bash
python --version  # deve ser 3.9 ou superior
pip --version
```

#### 2. Ollama (LLM Local)
**Instalar Ollama:**
- Windows/Mac: Baixe de [https://ollama.ai](https://ollama.ai)
- Linux: `curl -fsSL https://ollama.ai/install.sh | sh`

**Baixar um modelo:**
```bash
# Verificar se Ollama estÃ¡ rodando
ollama list

# Baixar modelo recomendado
ollama pull llama3

# Ou outros modelos disponÃ­veis:
# ollama pull llama2
# ollama pull mistral
# ollama pull phi
```

**Verificar instalaÃ§Ã£o:**
```bash
ollama run llama3 "Hello"
```

#### 3. Microsoft Visual C++ Build Tools (Somente Windows)
**NecessÃ¡rio para compilar algumas dependÃªncias Python**

- **OpÃ§Ã£o 1 (Recomendada):** Baixe e instale: [Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)
  - Durante instalaÃ§Ã£o, selecione "Desktop development with C++"
  
- **OpÃ§Ã£o 2:** Ative o Developer Mode no Windows
  - ConfiguraÃ§Ãµes â†’ AtualizaÃ§Ã£o e SeguranÃ§a â†’ Para desenvolvedores â†’ Modo de Desenvolvedor

> **Nota:** Se nÃ£o instalar, vocÃª pode ter erros ao instalar pacotes como `chroma-hnswlib`

### DependÃªncias Python CrÃ­ticas

O projeto usa as seguintes versÃµes especÃ­ficas para compatibilidade:

- **NumPy:** `1.26.4` (nÃ£o use NumPy 2.0+ - incompatÃ­vel com sentence-transformers)
- **LangChain:** Pacotes atualizados (`langchain-chroma`, `langchain-ollama`, `langchain-huggingface`)
- **ChromaDB:** `>=0.5.0` (corrige problemas de telemetria)
- **sentence-transformers:** Para embeddings locais

> **Importante:** As dependÃªncias serÃ£o instaladas automaticamente pelo `requirements.txt` com as versÃµes corretas.

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/patrickmcruz/rag-demo.git
cd rag-demo
```

### 2. Crie e ative um ambiente virtual
```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1

# Windows CMD:
.venv\Scripts\activate.bat

# Linux/Mac:
source .venv/bin/activate
```

> **Importante:** Sempre ative o ambiente virtual antes de executar comandos Python!

### 3. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

> **Nota:** A instalaÃ§Ã£o inclui:
> - LangChain e componentes atualizados
> - ChromaDB para vector store
> - Sentence Transformers para embeddings
> - NumPy 1.26.4 (compatÃ­vel)
> - Todas as dependÃªncias necessÃ¡rias

### 4. Configure as variÃ¡veis de ambiente
```bash
# Copie o arquivo de exemplo
# Windows:
copy .env.example .env

# Linux/Mac:
cp .env.example .env

# Edite .env conforme necessÃ¡rio (opcional)
```

**ConfiguraÃ§Ãµes principais no `.env`:**
```env
OLLAMA_MODEL=llama3              # Modelo Ollama a usar
VECTORSTORE_DIR=./vectorstore    # Onde salvar Ã­ndice
DATA_DIR=./data                  # Pasta com documentos
CHUNK_SIZE=500                   # Tamanho dos chunks
TOP_K_DOCUMENTS=3                # Documentos a recuperar
TEMPERATURE=0.0                  # Temperatura do LLM (0.0 = determinÃ­stico)
ANONYMIZED_TELEMETRY=False       # Desabilitar telemetria ChromaDB
```

### 5. Verifique a instalaÃ§Ã£o
```bash
# Verificar se Ollama estÃ¡ rodando
ollama list

# Testar imports Python
python -c "from src.ingest import ingest_documents; print('OK')"
```

## ğŸš€ Uso

### 1. Preparar documentos
Coloque seus documentos (PDF, TXT, MD) na pasta `data/`:
```bash
# Exemplo: copiar seus PDFs
cp seus_documentos.pdf data/

# Ou criar subpastas
mkdir data/contratos
cp *.pdf data/contratos/
```

### 2. Indexar documentos (IngestÃ£o)

**Usando a CLI (Recomendado):**
```bash
# Ativar ambiente virtual primeiro!
.\.venv\Scripts\Activate.ps1

# Indexar todos os documentos em data/
python main.py ingest

# OpÃ§Ãµes avanÃ§adas:
python main.py ingest --file-types pdf,txt --chunk-size 500 --chunk-overlap 50
```

**Usando Python diretamente:**
```python
from src.ingest import ingest_documents

# Indexar todos os documentos
vectorstore = ingest_documents(
    data_dir='./data',
    persist_dir='./vectorstore',
    file_types=['txt', 'pdf', 'md'],  # Tipos de arquivo
    chunk_size=500,                    # Tamanho dos chunks
    chunk_overlap=50                   # SobreposiÃ§Ã£o entre chunks
)

print("IndexaÃ§Ã£o concluÃ­da!")
```

> **Nota:** A primeira vez que rodar, o sistema baixarÃ¡ o modelo de embeddings (~90MB)

### 3. Consultar o sistema (Queries)

#### Modo Interativo (CLI)
```bash
# Iniciar modo interativo
python main.py query --interactive

# Exemplo de uso:
# > Quais sÃ£o os cargos do edital?
# > Qual o prazo de validade?
# > exit  (para sair)
```

#### Consulta Ãšnica
```bash
# Fazer uma pergunta direta
python main.py query -q "Qual o conteÃºdo do documento?"

# Com opÃ§Ãµes personalizadas:
python main.py query -q "Resumo" --top-k 5 --temperature 0.7 --model mistral
```

#### Modo ProgramÃ¡tico
```python
from src.chain import create_rag_chain
from src.query import RAGQuery

# Criar chain RAG
chain = create_rag_chain(
    vectorstore_path='./vectorstore',
    model_name='llama3',
    top_k=3,
    temperature=0.0
)

# Criar interface de query
query_interface = RAGQuery(chain, model_name='llama3')

# Fazer pergunta
response = query_interface.query("Qual Ã© o assunto principal?")

# Exibir resposta formatada
print(response)

# Ver estatÃ­sticas
stats = query_interface.get_stats()
print(f"Total de queries: {stats['total_queries']}")
print(f"Tempo mÃ©dio: {stats['avg_response_time']:.2f}s")
```

### 4. Adicionar novos documentos

Quando adicionar novos documentos, **re-indexe** para atualizar o vectorstore:

```bash
# 1. Adicionar novos arquivos em data/
cp novo_documento.pdf data/

# 2. Re-indexar
python main.py ingest

# O sistema criarÃ¡ um novo Ã­ndice com todos os documentos
```

### 5. Ver informaÃ§Ãµes do sistema

```bash
python main.py info
```

Exibe:
- Modelo LLM configurado
- Modelo de embeddings
- NÃºmero de documentos indexados
- LocalizaÃ§Ã£o do vectorstore

## ğŸ“ Estrutura do Projeto

```
rag-demo/
â”œâ”€â”€ .env                    # ConfiguraÃ§Ãµes (nÃ£o versionado)
â”œâ”€â”€ .env.example            # Exemplo de configuraÃ§Ãµes
â”œâ”€â”€ .gitignore              # Arquivos ignorados pelo git
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ README.md              # Este arquivo
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest.py          # Pipeline de ingestÃ£o
â”‚   â”œâ”€â”€ chain.py           # ConfiguraÃ§Ã£o da chain RAG
â”‚   â””â”€â”€ query.py           # Interface de consulta
â”‚
â”œâ”€â”€ data/                  # Documentos fonte
â”‚   â””â”€â”€ (seus arquivos)
â”‚
â”œâ”€â”€ vectorstore/           # Base vetorial persistida
â”‚   â””â”€â”€ (gerado automaticamente)
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_rag.py        # Testes unitÃ¡rios e integraÃ§Ã£o
```

## ğŸ§ª Testes

### Executar todos os testes
```bash
pytest tests/ -v
```

### Executar testes especÃ­ficos
```bash
# Apenas testes rÃ¡pidos
pytest tests/ -v -m "not slow"

# Testes de integraÃ§Ã£o
pytest tests/ -v -m integration

# Com cobertura
pytest tests/ --cov=src --cov-report=html
```

### Executar linting
```bash
# Formatar cÃ³digo
black src/ tests/

# Verificar estilo
flake8 src/ tests/

# Type checking
mypy src/
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de ambiente (.env)

```bash
# Modelo Ollama
OLLAMA_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434

# Modelo de Embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Caminhos
DATA_DIR=./data
VECTORSTORE_DIR=./vectorstore

# ConfiguraÃ§Ã£o RAG
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K_DOCUMENTS=3
TEMPERATURE=0.0

# Logging
LOG_LEVEL=INFO
```

### Modelos Ollama suportados
- `llama3` (recomendado)
- `phi3`
- `mistral`
- `codellama`

Para instalar um modelo:
```bash
ollama pull <model-name>
```

## ğŸ”® PrÃ³ximos Passos

### Curto prazo
- [ ] **SanitizaÃ§Ã£o de texto**: Implementar limpeza de documentos (remover caracteres especiais, normalizar Unicode, mÃºltiplos espaÃ§os)
- [ ] **Token-based splitting**: Substituir `RecursiveCharacterTextSplitter` por `TokenTextSplitter` para respeitar limites do modelo
- [ ] **ValidaÃ§Ã£o de chunks**: Garantir que chunks nÃ£o excedam 256 tokens do modelo de embeddings
- [ ] Adicionar suporte a mais formatos (DOCX, HTML)
- [ ] Implementar cache de embeddings para evitar reprocessamento
- [ ] Adicionar CLI com argparse
- [ ] Melhorar prompts para casos especÃ­ficos

### MÃ©dio prazo
- [ ] **PrÃ©-processamento avanÃ§ado**: OCR para PDFs escaneados, limpeza de headers/footers
- [ ] **Modelos de embedding alternativos**: Suporte para modelos multilÃ­ngues e otimizados para portuguÃªs
- [ ] **Chunking semÃ¢ntico**: DivisÃ£o por seÃ§Ãµes/parÃ¡grafos em vez de apenas tamanho
- [ ] Integrar LangSmith para observabilidade
- [ ] Adicionar avaliaÃ§Ã£o com RAGAS
- [ ] Implementar API REST com FastAPI
- [ ] Adicionar suporte a Vertex AI (produÃ§Ã£o)

### Longo prazo
- [ ] Interface web (Streamlit/Gradio)
- [ ] Suporte a conversas (chat com memÃ³ria)
- [ ] Multi-tenancy
- [ ] Deploy com Docker

## ğŸ“š DocumentaÃ§Ã£o Completa

Para documentaÃ§Ã£o detalhada, consulte o diretÃ³rio **[docs/](docs/)**:

### ğŸ“– Guias Essenciais
- **[InÃ­cio RÃ¡pido](docs/guides/quickstart.md)** - Configure o projeto em 10 minutos
- **[FAQ - Perguntas Frequentes](docs/FAQ.md)** - Respostas para dÃºvidas comuns
- **[Troubleshooting](docs/guides/troubleshooting.md)** - SoluÃ§Ãµes para problemas comuns

### ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada
- **[Guia de Modelos LLM](docs/guides/models.md)** - Escolha e configure modelos Ollama
- **[Guia de Embeddings](docs/guides/embeddings.md)** - Otimize embeddings para seu caso

### ğŸ—ï¸ Arquitetura e Desenvolvimento
- **[Arquitetura do Sistema](docs/ARCHITECTURE.md)** - Estrutura tÃ©cnica completa
- **[Changelog](docs/CHANGELOG.md)** - HistÃ³rico de mudanÃ§as

### ğŸ” Problemas Comuns (Resumo)

| Problema | SoluÃ§Ã£o RÃ¡pida |
|----------|----------------|
| Vector store not found | Execute `python main.py ingest` primeiro |
| Ollama 404 error | Instale o modelo: `ollama pull llama3` |
| ModuleNotFoundError | Ative ambiente virtual: `.\.venv\Scripts\Activate.ps1` |
| NumPy 2.0 error | `pip install "numpy==1.26.4" --force-reinstall` |

**Ver todas as soluÃ§Ãµes**: [Troubleshooting Completo](docs/guides/troubleshooting.md)

## Recursos Adicionais

### DocumentaÃ§Ã£o
- [LangChain Docs](https://python.langchain.com/)
- [Ollama](https://ollama.ai/)
- [Chroma](https://docs.trychroma.com/)
- [LangSmith](https://docs.smith.langchain.com/)

### Artigos relacionados
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:
1. FaÃ§a fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a GNU General Public License.

## âœï¸ Autor

**Patrick Motin Cruz**
AI Software Developer on IPPUC (Institute for Urban Research and Planning).
Graduate student in Data Science at UTFPR (Federal Technological University of ParanÃ¡).
2025

---

**Nota**: Este Ã© um projeto educacional/demonstrativo. Para uso em produÃ§Ã£o, considere adicionar autenticaÃ§Ã£o, rate limiting, monitoramento e outras funcionalidades enterprise.
