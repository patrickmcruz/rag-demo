# RAG Demo - Sistema RAG Profissional com LangChain

<div align="center">

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1.0-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

Sistema de Retrieval-Augmented Generation (RAG) profissional usando LangChain, Chroma e Ollama.

</div>

## ğŸ“‹ Ãndice

- [Sobre](#-sobre)
- [Arquitetura](#-arquitetura)
- [Funcionalidades](#-funcionalidades)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Uso](#-uso)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Testes](#-testes)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [PrÃ³ximos Passos](#-prÃ³ximos-passos)

## ğŸ¯ Sobre

Este projeto implementa um sistema RAG (Retrieval-Augmented Generation) completo e profissional, seguindo as melhores prÃ¡ticas de engenharia de software:

- âœ… **Modular e testÃ¡vel**: CÃ³digo organizado com separaÃ§Ã£o clara de responsabilidades
- âœ… **Logging e validaÃ§Ãµes**: Tratamento de erros robusto e logs informativos
- âœ… **Suporte multi-formato**: PDF, TXT, Markdown
- âœ… **ConfigurÃ¡vel**: VariÃ¡veis de ambiente para todas as configuraÃ§Ãµes
- âœ… **Documentado**: Docstrings completas e type hints
- âœ… **Preparado para produÃ§Ã£o**: Estrutura escalÃ¡vel e manutenÃ­vel

## ğŸ—ï¸ Arquitetura

```
[UsuÃ¡rio]
   â†“ (pergunta)
[Query Interface (query.py)]
   â†“
1. Embedding da pergunta
   â†“
2. Retrieval no Chroma (top-k documentos)
   â†“
3. Montagem do prompt com contexto
   â†“
4. Chamada ao LLM (Ollama)
   â†“
5. Resposta + metadados (fontes, tempo)
   â†“
[Resposta Estruturada]
```

### Componentes principais:

- **ingest.py**: Carrega, processa e indexa documentos
- **chain.py**: Define e configura a chain RAG
- **query.py**: Interface de alto nÃ­vel para consultas

## âœ¨ Funcionalidades

### Ingestion Pipeline
- âœ… Carregamento de mÃºltiplos formatos (TXT, PDF, MD)
- âœ… Splitting inteligente de documentos
- âœ… Embeddings com HuggingFace (sentence-transformers)
- âœ… IndexaÃ§Ã£o persistente com Chroma
- âœ… Logging detalhado de todo o processo

### RAG Chain
- âœ… ConfiguraÃ§Ã£o flexÃ­vel (temperatura, top-k, etc.)
- âœ… Suporte para mÃºltiplos modelos Ollama
- âœ… Prompts otimizados (PT/EN)
- âœ… ValidaÃ§Ãµes e tratamento de erros

### Query Interface
- âœ… CLI interativo
- âœ… Respostas estruturadas com metadados
- âœ… Rastreamento de fontes
- âœ… MÃ©tricas de performance
- âœ… HistÃ³rico de consultas

## ğŸ”§ PrÃ©-requisitos

### Sistema
- Python 3.9+
- 4GB+ RAM (para embeddings)
- ~2GB de espaÃ§o em disco

### Software necessÃ¡rio
1. **Ollama** (para LLM local)
   ```bash
   # Instalar Ollama: https://ollama.ai
   # Baixar modelo:
   ollama pull llama3
   ```

2. **Python e pip**
   ```bash
   python --version  # deve ser 3.9+
   ```

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/seu-usuario/rag-demo.git
cd rag-demo
```

### 2. Crie um ambiente virtual
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

### 3. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 4. Configure as variÃ¡veis de ambiente
```bash
# Copie o arquivo de exemplo
cp .env.example .env

# Edite .env conforme necessÃ¡rio
```

## ğŸš€ Uso

### 1. Preparar documentos
Coloque seus documentos na pasta `data/`:
```bash
# Adicione arquivos .txt, .pdf ou .md
cp seus_documentos.pdf data/
```

### 2. Indexar documentos
```python
python -c "
from src.ingest import ingest_documents

# Indexar todos os documentos
ingest_documents(
    data_dir='./data',
    persist_dir='./vectorstore',
    file_types=['txt', 'pdf', 'md']
)
"
```

### 3. Consultar o sistema

#### Modo interativo (CLI)
```python
from src.chain import create_rag_chain
from src.query import interactive_query_loop

# Criar chain
chain = create_rag_chain(vectorstore_path='./vectorstore')

# Iniciar CLI interativo
interactive_query_loop(chain)
```

#### Modo programÃ¡tico
```python
from src.chain import create_rag_chain
from src.query import RAGQuery

# Criar chain
chain = create_rag_chain(
    vectorstore_path='./vectorstore',
    model_name='llama3',
    top_k=3,
    temperature=0.0
)

# Criar interface de query
query = RAGQuery(chain, model_name='llama3')

# Fazer pergunta
response = query.query("Qual Ã© o assunto principal dos documentos?")
print(response)

# Ver estatÃ­sticas
print(query.get_stats())
```

## ğŸ“ Estrutura do Projeto

```
rag-demo/
â”œâ”€â”€ .env                    # ConfiguraÃ§Ãµes (nÃ£o versionado)
â”œâ”€â”€ .env.example            # Exemplo de configuraÃ§Ãµes
â”œâ”€â”€ .gitignore              # Arquivos ignorados pelo git
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ README.md              # Este arquivo
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest.py          # Pipeline de ingestÃ£o
â”‚   â”œâ”€â”€ chain.py           # ConfiguraÃ§Ã£o da chain RAG
â”‚   â””â”€â”€ query.py           # Interface de consulta
â”‚
â”œâ”€â”€ data/                  # Documentos fonte
â”‚   â””â”€â”€ (seus arquivos)
â”‚
â”œâ”€â”€ vectorstore/           # Base vetorial persistida
â”‚   â””â”€â”€ (gerado automaticamente)
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_rag.py        # Testes unitÃ¡rios e integraÃ§Ã£o
```

## ğŸ§ª Testes

### Executar todos os testes
```bash
pytest tests/ -v
```

### Executar testes especÃ­ficos
```bash
# Apenas testes rÃ¡pidos
pytest tests/ -v -m "not slow"

# Testes de integraÃ§Ã£o
pytest tests/ -v -m integration

# Com cobertura
pytest tests/ --cov=src --cov-report=html
```

### Executar linting
```bash
# Formatar cÃ³digo
black src/ tests/

# Verificar estilo
flake8 src/ tests/

# Type checking
mypy src/
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de ambiente (.env)

```bash
# Modelo Ollama
OLLAMA_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434

# Modelo de Embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Caminhos
DATA_DIR=./data
VECTORSTORE_DIR=./vectorstore

# ConfiguraÃ§Ã£o RAG
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K_DOCUMENTS=3
TEMPERATURE=0.0

# Logging
LOG_LEVEL=INFO
```

### Modelos Ollama suportados
- `llama3` (recomendado)
- `phi3`
- `mistral`
- `codellama`

Para instalar um modelo:
```bash
ollama pull <model-name>
```

## ğŸ”® PrÃ³ximos Passos

### Curto prazo
- [ ] Adicionar suporte a mais formatos (DOCX, HTML)
- [ ] Implementar cache de embeddings
- [ ] Adicionar CLI com argparse
- [ ] Melhorar prompts para casos especÃ­ficos

### MÃ©dio prazo
- [ ] Integrar LangSmith para observabilidade
- [ ] Adicionar avaliaÃ§Ã£o com RAGAS
- [ ] Implementar API REST com FastAPI
- [ ] Adicionar suporte a Vertex AI (produÃ§Ã£o)

### Longo prazo
- [ ] Interface web (Streamlit/Gradio)
- [ ] Suporte a conversas (chat com memÃ³ria)
- [ ] Multi-tenancy
- [ ] Deploy com Docker

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o
- [LangChain Docs](https://python.langchain.com/)
- [Ollama](https://ollama.ai/)
- [Chroma](https://docs.trychroma.com/)
- [LangSmith](https://docs.smith.langchain.com/)

### Artigos relacionados
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:
1. FaÃ§a fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## âœï¸ Autor

Desenvolvido com â¤ï¸ para demonstrar boas prÃ¡ticas em sistemas RAG.

---

**Nota**: Este Ã© um projeto educacional/demonstrativo. Para uso em produÃ§Ã£o, considere adicionar autenticaÃ§Ã£o, rate limiting, monitoramento e outras funcionalidades enterprise.
