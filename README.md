# RAG Demo - Sistema RAG Profissional com LangChain

<div align="center">

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1.0-green.svg)
![License](https://img.shields.io/badge/License-GNU%20GPL-blue)

Sistema de Retrieval-Augmented Generation (RAG) profissional usando LangChain, Chroma e Ollama.

</div>

## üìã √çndice

- [Sobre](#-sobre)
- [Arquitetura](#-arquitetura)
- [Funcionalidades](#-funcionalidades)
- [Pr√©-requisitos](#-pr√©-requisitos)
- [Instala√ß√£o](#-instala√ß√£o)
- [Uso](#-uso)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Testes](#-testes)
- [Configura√ß√£o](#-configura√ß√£o)
- [Pr√≥ximos Passos](#-pr√≥ximos-passos)
- [FAQ](#-faq)

## üéØ Sobre

Este projeto implementa um sistema RAG (Retrieval-Augmented Generation) completo e profissional, seguindo as melhores pr√°ticas de engenharia de software:

- **Modular e test√°vel**: C√≥digo organizado com separa√ß√£o clara de responsabilidades
- **Logging e valida√ß√µes**: Tratamento de erros robusto e logs informativos
- **Suporte multi-formato**: PDF, TXT, Markdown
- **Configur√°vel**: Vari√°veis de ambiente para todas as configura√ß√µes
- **Documentado**: Docstrings completas e type hints
- **Preparado para produ√ß√£o**: Estrutura escal√°vel e manuten√≠vel

## üèóÔ∏è Arquitetura

```
[Usu√°rio]
   ‚Üì (pergunta)
[Query Interface (query.py)]
   ‚Üì
1. Embedding da pergunta
   ‚Üì
2. Retrieval no Chroma (top-k documentos)
   ‚Üì
3. Montagem do prompt com contexto
   ‚Üì
4. Chamada ao LLM (Ollama)
   ‚Üì
5. Resposta + metadados (fontes, tempo)
   ‚Üì
[Resposta Estruturada]
```

### Componentes principais:

- **ingest.py**: Carrega, processa e indexa documentos
- **chain.py**: Define e configura a chain RAG
- **query.py**: Interface de alto n√≠vel para consultas

## ‚ú® Funcionalidades

### Ingestion Pipeline
- Carregamento de m√∫ltiplos formatos (TXT, PDF, MD)
- Splitting inteligente de documentos
- Embeddings com HuggingFace (sentence-transformers)
- Indexa√ß√£o persistente com Chroma
- Logging detalhado de todo o processo

### RAG Chain
- Configura√ß√£o flex√≠vel (temperatura, top-k, etc.)
- Suporte para m√∫ltiplos modelos Ollama
- Prompts otimizados (PT/EN)
- Valida√ß√µes e tratamento de erros

### Query Interface
- CLI interativo
- Respostas estruturadas com metadados
- Rastreamento de fontes
- M√©tricas de performance
- Hist√≥rico de consultas

## üîß Pr√©-requisitos

### Sistema
- **Python 3.9+** (testado com Python 3.12)
- **4GB+ RAM** (para embeddings e modelos)
- **~2GB de espa√ßo em disco** (para modelos e √≠ndices)
- **Windows 10/11, Linux ou macOS**

### Software Necess√°rio

#### 1. Python e pip
```bash
python --version  # deve ser 3.9 ou superior
pip --version
```

#### 2. Ollama (LLM Local)
**Instalar Ollama:**
- Windows/Mac: Baixe de [https://ollama.ai](https://ollama.ai)
- Linux: `curl -fsSL https://ollama.ai/install.sh | sh`

**Baixar um modelo:**
```bash
# Verificar se Ollama est√° rodando
ollama list

# Baixar modelo recomendado
ollama pull llama3

# Ou outros modelos dispon√≠veis:
# ollama pull llama2
# ollama pull mistral
# ollama pull phi
```

**Verificar instala√ß√£o:**
```bash
ollama run llama3 "Hello"
```

#### 3. Microsoft Visual C++ Build Tools (Somente Windows)
**Necess√°rio para compilar algumas depend√™ncias Python**

- **Op√ß√£o 1 (Recomendada):** Baixe e instale: [Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)
  - Durante instala√ß√£o, selecione "Desktop development with C++"
  
- **Op√ß√£o 2:** Ative o Developer Mode no Windows
  - Configura√ß√µes ‚Üí Atualiza√ß√£o e Seguran√ßa ‚Üí Para desenvolvedores ‚Üí Modo de Desenvolvedor

> **Nota:** Se n√£o instalar, voc√™ pode ter erros ao instalar pacotes como `chroma-hnswlib`

### Depend√™ncias Python Cr√≠ticas

O projeto usa as seguintes vers√µes espec√≠ficas para compatibilidade:

- **NumPy:** `1.26.4` (n√£o use NumPy 2.0+ - incompat√≠vel com sentence-transformers)
- **LangChain:** Pacotes atualizados (`langchain-chroma`, `langchain-ollama`, `langchain-huggingface`)
- **ChromaDB:** `>=0.5.0` (corrige problemas de telemetria)
- **sentence-transformers:** Para embeddings locais

> **Importante:** As depend√™ncias ser√£o instaladas automaticamente pelo `requirements.txt` com as vers√µes corretas.

## üì¶ Instala√ß√£o

### 1. Clone o reposit√≥rio
```bash
git clone https://github.com/patrickmcruz/rag-demo.git
cd rag-demo
```

### 2. Crie e ative um ambiente virtual
```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1

# Windows CMD:
.venv\Scripts\activate.bat

# Linux/Mac:
source .venv/bin/activate
```

> **Importante:** Sempre ative o ambiente virtual antes de executar comandos Python!

### 3. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

> **Nota:** A instala√ß√£o inclui:
> - LangChain e componentes atualizados
> - ChromaDB para vector store
> - Sentence Transformers para embeddings
> - NumPy 1.26.4 (compat√≠vel)
> - Todas as depend√™ncias necess√°rias

### 4. Configure as vari√°veis de ambiente
```bash
# Copie o arquivo de exemplo
# Windows:
copy .env.example .env

# Linux/Mac:
cp .env.example .env

# Edite .env conforme necess√°rio (opcional)
```

**Configura√ß√µes principais no `.env`:**
```env
OLLAMA_MODEL=llama3              # Modelo Ollama a usar
VECTORSTORE_DIR=./vectorstore    # Onde salvar √≠ndice
DATA_DIR=./data                  # Pasta com documentos
CHUNK_SIZE=500                   # Tamanho dos chunks
TOP_K_DOCUMENTS=3                # Documentos a recuperar
TEMPERATURE=0.0                  # Temperatura do LLM (0.0 = determin√≠stico)
ANONYMIZED_TELEMETRY=False       # Desabilitar telemetria ChromaDB
```

### 5. Verifique a instala√ß√£o
```bash
# Verificar se Ollama est√° rodando
ollama list

# Testar imports Python
python -c "from src.ingest import ingest_documents; print('OK')"
```

## üöÄ Uso

### 1. Preparar documentos
Coloque seus documentos (PDF, TXT, MD) na pasta `data/`:
```bash
# Exemplo: copiar seus PDFs
cp seus_documentos.pdf data/

# Ou criar subpastas
mkdir data/contratos
cp *.pdf data/contratos/
```

### 2. Indexar documentos (Ingest√£o)

**Usando a CLI (Recomendado):**
```bash
# Ativar ambiente virtual primeiro!
.\.venv\Scripts\Activate.ps1

# Indexar todos os documentos em data/
python main.py ingest

# Op√ß√µes avan√ßadas:
python main.py ingest --file-types pdf,txt --chunk-size 500 --chunk-overlap 50
```

**Usando Python diretamente:**
```python
from src.ingest import ingest_documents

# Indexar todos os documentos
vectorstore = ingest_documents(
    data_dir='./data',
    persist_dir='./vectorstore',
    file_types=['txt', 'pdf', 'md'],  # Tipos de arquivo
    chunk_size=500,                    # Tamanho dos chunks
    chunk_overlap=50                   # Sobreposi√ß√£o entre chunks
)

print("Indexa√ß√£o conclu√≠da!")
```

> **Nota:** A primeira vez que rodar, o sistema baixar√° o modelo de embeddings (~90MB)

### 3. Consultar o sistema (Queries)

#### Modo Interativo (CLI)
```bash
# Iniciar modo interativo
python main.py query --interactive

# Exemplo de uso:
# > Quais s√£o os cargos do edital?
# > Qual o prazo de validade?
# > exit  (para sair)
```

#### Consulta √önica
```bash
# Fazer uma pergunta direta
python main.py query -q "Qual o conte√∫do do documento?"

# Com op√ß√µes personalizadas:
python main.py query -q "Resumo" --top-k 5 --temperature 0.7 --model mistral
```

#### Modo Program√°tico
```python
from src.chain import create_rag_chain
from src.query import RAGQuery

# Criar chain RAG
chain = create_rag_chain(
    vectorstore_path='./vectorstore',
    model_name='llama3',
    top_k=3,
    temperature=0.0
)

# Criar interface de query
query_interface = RAGQuery(chain, model_name='llama3')

# Fazer pergunta
response = query_interface.query("Qual √© o assunto principal?")

# Exibir resposta formatada
print(response)

# Ver estat√≠sticas
stats = query_interface.get_stats()
print(f"Total de queries: {stats['total_queries']}")
print(f"Tempo m√©dio: {stats['avg_response_time']:.2f}s")
```

### 4. Adicionar novos documentos

Quando adicionar novos documentos, **re-indexe** para atualizar o vectorstore:

```bash
# 1. Adicionar novos arquivos em data/
cp novo_documento.pdf data/

# 2. Re-indexar
python main.py ingest

# O sistema criar√° um novo √≠ndice com todos os documentos
```

### 5. Ver informa√ß√µes do sistema

```bash
python main.py info
```

Exibe:
- Modelo LLM configurado
- Modelo de embeddings
- N√∫mero de documentos indexados
- Localiza√ß√£o do vectorstore

## üìÅ Estrutura do Projeto

```
rag-demo/
‚îú‚îÄ‚îÄ .env                    # Configura√ß√µes (n√£o versionado)
‚îú‚îÄ‚îÄ .env.example            # Exemplo de configura√ß√µes
‚îú‚îÄ‚îÄ .gitignore              # Arquivos ignorados pelo git
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md              # Este arquivo
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py          # Pipeline de ingest√£o
‚îÇ   ‚îú‚îÄ‚îÄ chain.py           # Configura√ß√£o da chain RAG
‚îÇ   ‚îî‚îÄ‚îÄ query.py           # Interface de consulta
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Documentos fonte
‚îÇ   ‚îî‚îÄ‚îÄ (seus arquivos)
‚îÇ
‚îú‚îÄ‚îÄ vectorstore/           # Base vetorial persistida
‚îÇ   ‚îî‚îÄ‚îÄ (gerado automaticamente)
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ test_rag.py        # Testes unit√°rios e integra√ß√£o
```

## üß™ Testes

### Executar todos os testes
```bash
pytest tests/ -v
```

### Executar testes espec√≠ficos
```bash
# Apenas testes r√°pidos
pytest tests/ -v -m "not slow"

# Testes de integra√ß√£o
pytest tests/ -v -m integration

# Com cobertura
pytest tests/ --cov=src --cov-report=html
```

### Executar linting
```bash
# Formatar c√≥digo
black src/ tests/

# Verificar estilo
flake8 src/ tests/

# Type checking
mypy src/
```

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de ambiente (.env)

```bash
# Modelo Ollama
OLLAMA_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434

# Modelo de Embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Caminhos
DATA_DIR=./data
VECTORSTORE_DIR=./vectorstore

# Configura√ß√£o RAG
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K_DOCUMENTS=3
TEMPERATURE=0.0

# Logging
LOG_LEVEL=INFO
```

### Modelos Ollama suportados
- `llama3` (recomendado)
- `phi3`
- `mistral`
- `codellama`

Para instalar um modelo:
```bash
ollama pull <model-name>
```

## üîÆ Pr√≥ximos Passos

### Curto prazo
- [ ] **Sanitiza√ß√£o de texto**: Implementar limpeza de documentos (remover caracteres especiais, normalizar Unicode, m√∫ltiplos espa√ßos)
- [ ] **Token-based splitting**: Substituir `RecursiveCharacterTextSplitter` por `TokenTextSplitter` para respeitar limites do modelo
- [ ] **Valida√ß√£o de chunks**: Garantir que chunks n√£o excedam 256 tokens do modelo de embeddings
- [ ] Adicionar suporte a mais formatos (DOCX, HTML)
- [ ] Implementar cache de embeddings para evitar reprocessamento
- [ ] Adicionar CLI com argparse
- [ ] Melhorar prompts para casos espec√≠ficos

### M√©dio prazo
- [ ] **Pr√©-processamento avan√ßado**: OCR para PDFs escaneados, limpeza de headers/footers
- [ ] **Modelos de embedding alternativos**: Suporte para modelos multil√≠ngues e otimizados para portugu√™s
- [ ] **Chunking sem√¢ntico**: Divis√£o por se√ß√µes/par√°grafos em vez de apenas tamanho
- [ ] Integrar LangSmith para observabilidade
- [ ] Adicionar avalia√ß√£o com RAGAS
- [ ] Implementar API REST com FastAPI
- [ ] Adicionar suporte a Vertex AI (produ√ß√£o)

### Longo prazo
- [ ] Interface web (Streamlit/Gradio)
- [ ] Suporte a conversas (chat com mem√≥ria)
- [ ] Multi-tenancy
- [ ] Deploy com Docker

## üîç Troubleshooting

### Problemas Comuns e Solu√ß√µes

#### 1. Erro: "Microsoft Visual C++ 14.0 or greater is required"
**Problema:** Ao instalar depend√™ncias no Windows, falta compilador C++.

**Solu√ß√£o:**
- Instale [Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)
- OU remova `chroma-hnswlib` do requirements.txt (n√£o √© obrigat√≥rio)

#### 2. Erro: "np.float_ was removed in NumPy 2.0"
**Problema:** Incompatibilidade entre NumPy 2.0+ e sentence-transformers.

**Solu√ß√£o:**
```bash
pip install "numpy==1.26.4" --force-reinstall
```

#### 3. Erro: "Vector store not found"
**Problema:** Tentando fazer query antes de indexar documentos.

**Solu√ß√£o:**
```bash
# Primeiro indexe os documentos
python main.py ingest

# Depois fa√ßa queries
python main.py query -q "sua pergunta"
```

#### 4. Erro: "Ollama call failed with status code 404"
**Problema:** Modelo Ollama n√£o est√° instalado.

**Solu√ß√£o:**
```bash
# Verificar modelos instalados
ollama list

# Instalar modelo necess√°rio
ollama pull llama3
```

#### 5. Erro: "ModuleNotFoundError: No module named 'langchain_community'"
**Problema:** Ambiente virtual n√£o est√° ativado ou depend√™ncias n√£o foram instaladas.

**Solu√ß√£o:**
```bash
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1

# Reinstalar depend√™ncias
pip install -r requirements.txt
```

#### 6. Warnings de Deprecation do LangChain
**Problema:** Usando vers√µes antigas de pacotes LangChain.

**Solu√ß√£o:** As vers√µes corretas j√° est√£o no `requirements.txt`:
- `langchain-chroma` (n√£o `langchain_community.vectorstores`)
- `langchain-ollama` (n√£o `langchain_community.llms`)
- `langchain-huggingface` (n√£o `langchain_community.embeddings`)

#### 7. ChromaDB Telemetry Errors
**Problema:** Erros de telemetria do ChromaDB no console.

**Solu√ß√£o:** J√° configurado no c√≥digo para desabilitar telemetria automaticamente.

#### 8. Certificado SSL em ambientes corporativos
**Problema:** Erros de certificado ao baixar modelos.

**Solu√ß√£o:**
```bash
# Temporariamente (n√£o recomendado em produ√ß√£o)
set CURL_CA_BUNDLE=
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
```

### Dicas de Performance

**Melhorar velocidade de resposta:**
- Use modelos menores: `ollama pull phi` ou `ollama pull mistral`
- Reduza `top_k` para 2 ou 1
- Configure `temperature=0.0` para respostas mais r√°pidas

**Economizar mem√≥ria:**
- Use chunks menores: `CHUNK_SIZE=300`
- Processe menos documentos por vez

**Melhorar qualidade das respostas:**
- Aumente `top_k` para 5-7
- Use `chunk_overlap` maior: `100`
- Teste diferentes modelos Ollama

## ‚ùì FAQ

Perguntas frequentes sobre o projeto? Consulte o **[FAQ.md](FAQ.md)** para:

- Como funciona a sanitiza√ß√£o e tokeniza√ß√£o?
- Que melhorias implementar na pipeline?
- Troubleshooting de problemas comuns
- Otimiza√ß√£o de performance
- Escolha de modelos de embedding

## Recursos Adicionais

### Documenta√ß√£o
- [LangChain Docs](https://python.langchain.com/)
- [Ollama](https://ollama.ai/)
- [Chroma](https://docs.trychroma.com/)
- [LangSmith](https://docs.smith.langchain.com/)

### Artigos relacionados
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:
1. Fa√ßa fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa GNU General Public License.

## ‚úçÔ∏è Autor

**Patrick Motin Cruz**
AI Software Developer on IPPUC (Institute for Urban Research and Planning).
Graduate student in Data Science at UTFPR (Federal Technological University of Paran√°).
2025

---

**Nota**: Este √© um projeto educacional/demonstrativo. Para uso em produ√ß√£o, considere adicionar autentica√ß√£o, rate limiting, monitoramento e outras funcionalidades enterprise.
