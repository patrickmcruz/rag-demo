# Arquitetura do Sistema RAG Demo

Este documento descreve a arquitetura t√©cnica completa do sistema RAG (Retrieval-Augmented Generation).

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Componentes Principais](#componentes-principais)
- [Fluxo de Dados](#fluxo-de-dados)
- [Stack Tecnol√≥gica](#stack-tecnol√≥gica)
- [Decis√µes de Design](#decis√µes-de-design)

---

## üéØ Vis√£o Geral

O **RAG Demo** √© um sistema de recupera√ß√£o e gera√ß√£o aumentada que permite fazer perguntas sobre documentos usando um LLM local. A arquitetura segue o padr√£o RAG moderno com tr√™s fases principais:

1. **Ingest√£o**: Processar e indexar documentos
2. **Recupera√ß√£o**: Buscar contexto relevante
3. **Gera√ß√£o**: Produzir respostas com LLM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SISTEMA RAG DEMO                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  INGEST√ÉO    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  INDEXA√á√ÉO   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ VECTORSTORE  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (ingest.py) ‚îÇ    ‚îÇ  (ChromaDB)  ‚îÇ    ‚îÇ  (Chroma)    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                         ‚ñ≤         ‚îÇ
‚îÇ         ‚îÇ                                         ‚îÇ         ‚îÇ
‚îÇ         ‚ñº                                         ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  DOCUMENTOS  ‚îÇ                                ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (PDF/TXT/MD) ‚îÇ                                ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ         ‚îÇ
‚îÇ                                                   ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   QUERY      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  RETRIEVAL   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ  ‚îÇ  (query.py)  ‚îÇ    ‚îÇ  (chain.py)  ‚îÇ                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ                             ‚îÇ
‚îÇ         ‚îÇ                     ‚ñº                             ‚îÇ
‚îÇ         ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ     LLM      ‚îÇ                      ‚îÇ
‚îÇ         ‚îÇ            ‚îÇ   (Ollama)   ‚îÇ                      ‚îÇ
‚îÇ         ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ         ‚îÇ                     ‚îÇ                             ‚îÇ
‚îÇ         ‚ñº                     ‚ñº                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ         RESPOSTA FINAL          ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ   (answer + sources + metadata) ‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üß© Componentes Principais

### 1. Pipeline de Ingest√£o (`src/ingest.py`)

**Responsabilidade**: Processar documentos e criar √≠ndice vetorial.

#### Classe: `DocumentIngestor`

```python
class DocumentIngestor:
    """Handles document ingestion and indexing for RAG."""
    
    def __init__(self, embedding_model, chunk_size, chunk_overlap):
        # Configura√ß√£o do modelo de embeddings e par√¢metros
        
    def load_documents(self, data_dir, file_types) -> List[Document]:
        # Carrega documentos de m√∫ltiplos formatos
        
    def split_documents(self, documents) -> List[Document]:
        # Divide documentos em chunks menores
        
    def create_vectorstore(self, documents, persist_dir) -> Chroma:
        # Cria e persiste vector store
        
    def ingest(self, data_dir, persist_dir, file_types) -> Chroma:
        # Pipeline completa de ingest√£o
```

#### Fluxo de Ingest√£o

```
Documentos (data/)
      ‚Üì
[DirectoryLoader] ‚Üí Carrega arquivos (.pdf, .txt, .md)
      ‚Üì
[RecursiveCharacterTextSplitter] ‚Üí Divide em chunks
      ‚Üì
[HuggingFaceEmbeddings] ‚Üí Gera embeddings
      ‚Üì
[ChromaDB] ‚Üí Indexa e persiste
      ‚Üì
VectorStore (vectorstore/)
```

**Par√¢metros de Splitting**:
- `chunk_size`: 500 caracteres (balanceio contexto/granularidade)
- `chunk_overlap`: 50 caracteres (evita perda de contexto)
- `separators`: `["\n\n", "\n", " ", ""]` (hier√°rquico)

### 2. RAG Chain (`src/chain.py`)

**Responsabilidade**: Configurar e executar a chain de recupera√ß√£o e gera√ß√£o.

#### Classe: `RAGChainBuilder`

```python
class RAGChainBuilder:
    """Builder for creating configurable RAG chains."""
    
    def __init__(self, vectorstore_path, model_name, embedding_model, 
                 temperature, top_k):
        # Configura√ß√£o da chain
        
    def build_retriever(self):
        # Configura retriever do vectorstore
        
    def build_llm(self):
        # Inicializa LLM (Ollama)
        
    def build_prompt(self, language) -> ChatPromptTemplate:
        # Cria prompt template otimizado
        
    def build(self) -> Runnable:
        # Monta chain completa
```

#### Arquitetura da Chain

```
Query do Usu√°rio
      ‚Üì
[Embedding] ‚Üí Vetoriza pergunta
      ‚Üì
[Retriever] ‚Üí Busca top-k documentos similares
      ‚Üì
[Contexto + Query] ‚Üí Monta prompt
      ‚Üì
[LLM Ollama] ‚Üí Gera resposta
      ‚Üì
[Output Parser] ‚Üí Extrai texto
      ‚Üì
Resposta Final
```

**Componentes LangChain**:
```python
chain = (
    RunnableParallel({
        "context": retriever,
        "question": RunnablePassthrough()
    })
    | prompt
    | llm
    | StrOutputParser()
)
```

### 3. Interface de Query (`src/query.py`)

**Responsabilidade**: Interface de alto n√≠vel para consultas.

#### Classe: `RAGQuery`

```python
class RAGQuery:
    """High-level interface for querying the RAG system."""
    
    def __init__(self, chain, model_name):
        # Inicializa interface
        
    def query(self, question: str) -> RAGResponse:
        # Executa query e retorna resposta estruturada
        
    def get_stats(self) -> Dict:
        # Retorna estat√≠sticas de uso
```

#### Dataclass: `RAGResponse`

```python
@dataclass
class RAGResponse:
    answer: str                      # Resposta gerada
    sources: List[Document]          # Documentos fonte
    query: str                       # Query original
    response_time: float             # Tempo de resposta
    model_name: str                  # Modelo usado
    retrieval_scores: List[float]    # Scores de similaridade
```

### 4. CLI Principal (`main.py`)

**Responsabilidade**: Interface de linha de comando.

#### Comandos

```bash
# Ingest√£o
python main.py ingest [--file-types] [--chunk-size] [--chunk-overlap]

# Query
python main.py query [-q QUESTION | --interactive] 
                    [--top-k] [--temperature]

# Info
python main.py info
```

#### Argumentos Globais

```python
--data-dir          # Diret√≥rio com documentos
--vectorstore-dir   # Diret√≥rio do vectorstore
--model            # Modelo Ollama a usar
```

---

## üîÑ Fluxo de Dados

### Fase 1: Ingest√£o (Offline)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Documentos ‚îÇ
‚îÇ (PDF/TXT)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Load Documents      ‚îÇ
‚îÇ - PyPDFLoader       ‚îÇ
‚îÇ - TextLoader        ‚îÇ
‚îÇ - UnstructuredMD    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Split into Chunks   ‚îÇ
‚îÇ - Size: 500 chars   ‚îÇ
‚îÇ - Overlap: 50 chars ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Generate Embeddings ‚îÇ
‚îÇ - all-MiniLM-L6-v2  ‚îÇ
‚îÇ - 384 dimensions    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Index in ChromaDB   ‚îÇ
‚îÇ - HNSW algorithm    ‚îÇ
‚îÇ - Cosine similarity ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Persist to Disk     ‚îÇ
‚îÇ ./vectorstore/      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fase 2: Query (Online)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Query    ‚îÇ
‚îÇ  "..."     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Embed Query         ‚îÇ
‚îÇ - Same model        ‚îÇ
‚îÇ - 384 dims          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Similarity Search   ‚îÇ
‚îÇ - Cosine distance   ‚îÇ
‚îÇ - Top-k=3 docs      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Build Prompt        ‚îÇ
‚îÇ - Context + Query   ‚îÇ
‚îÇ - Template PT/EN    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Call LLM (Ollama)   ‚îÇ
‚îÇ - Llama 3 local     ‚îÇ
‚îÇ - Temperature: 0.0  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Parse Response      ‚îÇ
‚îÇ - Extract answer    ‚îÇ
‚îÇ - Add metadata      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Return RAGResponse  ‚îÇ
‚îÇ - Answer + Sources  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Stack Tecnol√≥gica

### Core Framework
- **LangChain 1.1.0**: Orquestra√ß√£o da pipeline RAG
  - `langchain-core`: Abstra√ß√µes base
  - `langchain-chroma`: Integra√ß√£o ChromaDB
  - `langchain-ollama`: Integra√ß√£o Ollama
  - `langchain-huggingface`: Embeddings
  - `langchain-community`: Loaders e utilit√°rios

### Vector Store
- **ChromaDB >= 0.5.0**: Banco vetorial
  - Algoritmo: HNSW (Hierarchical Navigable Small World)
  - M√©trica: Similaridade por cosseno
  - Persist√™ncia: Disco local
  - Performance: O(log n) para busca

### Embeddings
- **HuggingFace Sentence Transformers**
  - Modelo: `all-MiniLM-L6-v2`
  - Dimens√µes: 384
  - Tamanho: ~90MB
  - Velocidade: ~50 senten√ßas/segundo (CPU)
  - Multil√≠ngue: Suporte PT-BR

### LLM
- **Ollama**: Runtime local para LLMs
  - Modelo padr√£o: Llama 3 (8B)
  - Alternativas: Mistral, Phi, Llama 2
  - Quantiza√ß√£o: 4-bit (Q4_0)
  - Interface: REST API local (port 11434)

### Processamento de Documentos
- **pypdf 3.17.4**: Extra√ß√£o de PDFs
- **unstructured 0.11.8**: Parser multi-formato
- **python-magic-bin**: Detec√ß√£o de tipos (Windows)

### Infraestrutura
- **Python 3.9+**: Linguagem base
- **NumPy 1.26.4**: Opera√ß√µes vetoriais (fixado para compatibilidade)
- **python-dotenv**: Gerenciamento de configura√ß√£o

---

## üé® Decis√µes de Design

### 1. **Por que Ollama + Llama 3?**

**Vantagens**:
- ‚úÖ **100% Local**: Privacidade total, sem envio de dados
- ‚úÖ **Zero Custo**: Sem custos de API
- ‚úÖ **Offline**: Funciona sem internet
- ‚úÖ **Flex√≠vel**: F√°cil trocar modelos
- ‚úÖ **Open Source**: Transpar√™ncia total

**Trade-offs**:
- ‚ö†Ô∏è Requer hardware local (GPU recomendada)
- ‚ö†Ô∏è Mais lento que APIs cloud
- ‚ö†Ô∏è Context window menor que GPT-4

### 2. **Por que ChromaDB?**

**Vantagens**:
- ‚úÖ **Simples**: API Python nativa
- ‚úÖ **Persistente**: Salva em disco automaticamente
- ‚úÖ **R√°pido**: HNSW algorithm eficiente
- ‚úÖ **Leve**: Sem servidor separado necess√°rio
- ‚úÖ **Integrado**: Suporte nativo LangChain

**Alternativas consideradas**:
- ~~FAISS~~: Sem persist√™ncia nativa
- ~~Pinecone~~: Pago, cloud-only
- ~~Weaviate~~: Complexo para uso local

### 3. **Por que all-MiniLM-L6-v2?**

**Vantagens**:
- ‚úÖ **Pequeno**: ~90MB (vs. 1GB+ de modelos maiores)
- ‚úÖ **R√°pido**: Embeddings em tempo real
- ‚úÖ **Multil√≠ngue**: Bom suporte PT-BR
- ‚úÖ **Qualidade**: Performance competitiva
- ‚úÖ **Popular**: Bem testado e documentado

**Benchmark**:
```
Modelo              | Tamanho | Dimens√µes | Velocidade | Qualidade
--------------------|---------|-----------|------------|----------
all-MiniLM-L6-v2    | 90MB    | 384       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | ‚≠ê‚≠ê‚≠ê‚≠ê
all-mpnet-base-v2   | 438MB   | 768       | ‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
multilingual-e5     | 560MB   | 768       | ‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```

### 4. **Chunk Size: 500 caracteres**

**Raz√£o**: Balanceio entre contexto e granularidade

```python
# Muito pequeno (200):
# ‚ùå Perde contexto
# ‚úÖ Busca precisa

# Muito grande (1000):
# ‚úÖ Mant√©m contexto
# ‚ùå Busca imprecisa

# Ideal (500):
# ‚úÖ Contexto suficiente
# ‚úÖ Granularidade boa
# ‚úÖ Performance balanceada
```

### 5. **Top-K = 3 documentos**

**Raz√£o**: Sweet spot entre contexto e ru√≠do

```python
# K = 1:  Pode perder contexto importante
# K = 3:  Contexto suficiente sem ru√≠do ‚úÖ
# K = 5:  Mais contexto, mas pode ter irrelevante
# K = 10: Muito ru√≠do, confunde o LLM
```

### 6. **Temperature = 0.0**

**Raz√£o**: Respostas determin√≠sticas e factuais

```python
# Temperature 0.0:
# ‚úÖ Respostas consistentes
# ‚úÖ Mais factual
# ‚úÖ Menos alucina√ß√µes
# ‚ùå Menos criativo

# Temperature 0.7+:
# ‚úÖ Mais criativo
# ‚ùå Menos consistente
# ‚ùå Mais alucina√ß√µes
```

---

## üìä Performance e Escalabilidade

### M√©tricas Atuais

| Opera√ß√£o | Tempo | Throughput |
|----------|-------|------------|
| Ingest√£o (1 PDF, 50 p√°ginas) | ~10s | ~5 p√°ginas/s |
| Embedding (por documento) | ~100ms | ~10 docs/s |
| Query (top-3) | ~1-3s | Depende do LLM |
| Indexa√ß√£o (100 chunks) | ~2s | ~50 chunks/s |

### Bottlenecks

1. **LLM (Ollama)**: Principal gargalo (~1-2s por query)
   - **Solu√ß√£o**: GPU, modelos menores (phi), quantiza√ß√£o
   
2. **Embeddings**: Segundo gargalo (~100ms por chunk)
   - **Solu√ß√£o**: Cache, batch processing, GPU
   
3. **I/O**: Carregamento de PDFs
   - **Solu√ß√£o**: Processamento paralelo

### Escalabilidade

**Documentos**:
- ‚úÖ Atual: ~100-1000 documentos (testado)
- ‚úÖ Estimado: ~10,000 documentos (sem re-arquitetura)
- ‚ö†Ô∏è >10,000: Considerar Pinecone ou Weaviate

**Queries**:
- ‚úÖ Atual: 1 usu√°rio (CLI local)
- ‚úÖ Poss√≠vel: ~10 usu√°rios simult√¢neos (API REST)
- ‚ö†Ô∏è >100: Requer load balancing e cache

---

## üîê Seguran√ßa e Privacidade

### Princ√≠pios

1. **Local-First**: Todos dados permanecem na m√°quina
2. **Zero Cloud**: Sem envio de dados para servi√ßos externos
3. **Open Source**: C√≥digo audit√°vel
4. **Telemetry Off**: ChromaDB telemetry desabilitada

### Considera√ß√µes

- ‚úÖ **Documentos sens√≠veis**: Seguros (n√£o saem da m√°quina)
- ‚úÖ **Queries privadas**: N√£o logadas externamente
- ‚ö†Ô∏è **Logs locais**: Podem conter informa√ß√µes sens√≠veis
- ‚ö†Ô∏è **Vectorstore**: Cont√©m chunks dos documentos (criptografar disco)

---

## üîÆ Futuras Melhorias

### Curto Prazo
- [ ] Cache de embeddings (evitar re-embedding)
- [ ] Batch processing para ingest√£o
- [ ] M√©tricas de qualidade (ragas)

### M√©dio Prazo
- [ ] API REST com FastAPI
- [ ] Interface web (Streamlit/Gradio)
- [ ] Suporte para mais formatos (DOCX, HTML)
- [ ] Multi-tenancy (m√∫ltiplos vectorstores)

### Longo Prazo
- [ ] Fine-tuning de embeddings
- [ ] Hybrid search (vetorial + keyword)
- [ ] Re-ranking com cross-encoder
- [ ] Streaming de respostas

---

## üìö Refer√™ncias

- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Ollama Documentation](https://ollama.ai/)
- [RAG Best Practices (Pinecone)](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

**Vers√£o do Documento**: 1.0.0  
**√öltima Atualiza√ß√£o**: Novembro 2025
