# Guia de Embeddings

Entenda, configure e otimize os embeddings no RAG Demo.

## üìã O que s√£o Embeddings?

**Embeddings** s√£o representa√ß√µes vetoriais (num√©ricas) de texto que capturam o significado sem√¢ntico.

```
Texto: "O gato est√° no telhado"
                ‚Üì
Embedding: [0.234, -0.891, 0.456, ..., 0.123]
           (vetor de 384 n√∫meros)
```

**Por que s√£o importantes no RAG?**
- üîç Permitem busca sem√¢ntica (n√£o apenas keywords)
- üìä Medem similaridade entre textos
- ‚ö° Permitem recupera√ß√£o r√°pida (busca vetorial)

## üéØ Modelo Atual: all-MiniLM-L6-v2

### Especifica√ß√µes

```yaml
Nome: sentence-transformers/all-MiniLM-L6-v2
Tamanho: ~90MB
Dimens√µes: 384
Tipo: Sentence Transformer
Base: MiniLM (Microsoft)
Treinamento: 1 bilh√£o de pares de senten√ßas
Licen√ßa: Apache 2.0
```

### Caracter√≠sticas

**‚úÖ Vantagens:**
- Pequeno e r√°pido (~50 senten√ßas/segundo em CPU)
- Boa qualidade para uso geral
- Suporte multil√≠ngue (incluindo portugu√™s)
- Bem documentado e testado
- Gratuito e open source

**‚ö†Ô∏è Limita√ß√µes:**
- N√£o especializado (gen√©rico)
- 384 dimens√µes (vs. 768 de modelos maiores)
- Performance em portugu√™s n√£o √© perfeita

### Benchmark

| Tarefa | Score | Rank |
|--------|-------|------|
| Similaridade Sem√¢ntica | 78.9% | Top 15% |
| Classifica√ß√£o | 76.2% | Top 20% |
| Clustering | 72.1% | Top 25% |

## üîÑ Como Funcionam no RAG Demo

### 1. Fase de Ingest√£o

```python
# C√≥digo simplificado
from langchain_huggingface import HuggingFaceEmbeddings

# Carregar modelo
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2"
)

# Para cada chunk de documento:
chunk = "Este √© um trecho do documento..."
vector = embeddings.embed_query(chunk)
# vector = [0.234, -0.891, ..., 0.123] (384 n√∫meros)

# Salvar no ChromaDB
vectorstore.add(chunk, vector)
```

### 2. Fase de Query

```python
# Query do usu√°rio
query = "Qual o conte√∫do do documento?"

# Embedding da query (mesmo modelo!)
query_vector = embeddings.embed_query(query)

# Buscar chunks similares (cosine similarity)
results = vectorstore.similarity_search(query_vector, k=3)
```

### 3. C√°lculo de Similaridade

```python
# Similaridade por cosseno
similarity = cosine_similarity(query_vector, chunk_vector)

# Valores:
# 1.0  = id√™ntico
# 0.8+ = muito similar
# 0.6+ = similar
# 0.4- = pouco similar
```

## üé® Modelos Alternativos

### Compara√ß√£o de Modelos

| Modelo | Tamanho | Dims | Velocidade | Qualidade | Multil√≠ngue |
|--------|---------|------|------------|-----------|-------------|
| **all-MiniLM-L6-v2** | 90MB | 384 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| all-mpnet-base-v2 | 438MB | 768 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| multilingual-e5-base | 560MB | 768 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| paraphrase-multilingual | 470MB | 768 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### 1. all-mpnet-base-v2 (Melhor Qualidade)

**Quando usar**: Qualidade √© mais importante que velocidade

```python
# src/ingest.py ou .env
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
```

**Caracter√≠sticas**:
- 768 dimens√µes (melhor representa√ß√£o)
- Melhor performance em benchmarks
- 5x mais lento que MiniLM
- ~440MB

### 2. multilingual-e5-base (Melhor Multil√≠ngue)

**Quando usar**: Documentos em m√∫ltiplos idiomas ou portugu√™s predominante

```python
EMBEDDING_MODEL=intfloat/multilingual-e5-base
```

**Caracter√≠sticas**:
- Treinado em 100+ idiomas
- Excelente para portugu√™s
- 768 dimens√µes
- ~560MB

### 3. paraphrase-multilingual-MiniLM-L12-v2 (Balanceado)

**Quando usar**: Meio-termo entre qualidade e velocidade

```python
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
```

**Caracter√≠sticas**:
- 384 dimens√µes
- Melhor multil√≠ngue que all-MiniLM
- ~120MB
- Bom compromisso

## ‚öôÔ∏è Trocar Modelo de Embedding

### Op√ß√£o 1: Via .env

```env
# .env
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
```

### Op√ß√£o 2: Via C√≥digo

```python
# src/ingest.py - linha ~35
def __init__(
    self,
    embedding_model: str = "all-mpnet-base-v2",  # Alterar aqui
    chunk_size: int = 500,
    chunk_overlap: int = 50,
):
```

### Op√ß√£o 3: Via CLI (Futuro)

```bash
python main.py ingest --embedding-model all-mpnet-base-v2
```

### ‚ö†Ô∏è IMPORTANTE

**Sempre re-indexe** ap√≥s trocar modelo:

```bash
# 1. Deletar vectorstore antigo
rm -rf vectorstore/

# 2. Re-indexar com novo modelo
python main.py ingest
```

**Por qu√™?** Embeddings de modelos diferentes s√£o incompat√≠veis!

## üîç Otimiza√ß√£o de Performance

### 1. Usar GPU (se dispon√≠vel)

```python
# src/ingest.py
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': 'cuda'}  # ou 'mps' para Mac M1/M2
)
```

**Ganho**: 5-10x mais r√°pido

### 2. Batch Processing

```python
# Embeddings em lote (mais eficiente)
texts = [chunk1, chunk2, chunk3, ...]
vectors = embeddings.embed_documents(texts)  # Todos de uma vez
```

**Ganho**: 2-3x mais r√°pido que um por um

### 3. Cache de Embeddings

```python
# Salvar embeddings calculados
import pickle

embeddings_cache = {}
for chunk in chunks:
    if chunk not in embeddings_cache:
        embeddings_cache[chunk] = embeddings.embed_query(chunk)
    vector = embeddings_cache[chunk]
```

**Ganho**: Instant√¢neo para documentos repetidos

## üìä Avalia√ß√£o de Qualidade

### Teste Manual

```python
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Textos similares
text1 = "O cachorro corre no parque"
text2 = "Um c√£o est√° correndo no jardim"
text3 = "Python √© uma linguagem de programa√ß√£o"

v1 = embeddings.embed_query(text1)
v2 = embeddings.embed_query(text2)
v3 = embeddings.embed_query(text3)

# Calcular similaridade
from numpy import dot
from numpy.linalg import norm

def cosine_sim(a, b):
    return dot(a, b) / (norm(a) * norm(b))

print(f"text1 <-> text2: {cosine_sim(v1, v2):.3f}")  # ~0.75 (similar)
print(f"text1 <-> text3: {cosine_sim(v1, v3):.3f}")  # ~0.15 (diferente)
```

### Benchmarks Autom√°ticos

Use **RAGAS** (Retrieval-Augmented Generation Assessment):

```bash
pip install ragas

# TODO: Implementar avalia√ß√£o autom√°tica
```

## üéØ Escolhendo o Modelo Certo

### Casos de Uso

#### üìÑ Documentos Gerais (Contratos, Editais)
‚Üí **all-MiniLM-L6-v2** (padr√£o)
- R√°pido e eficiente
- Boa qualidade geral

#### üåê M√∫ltiplos Idiomas
‚Üí **multilingual-e5-base**
- Melhor para PT-BR
- Suporta 100+ idiomas

#### üéì Documentos T√©cnicos/Acad√™micos
‚Üí **all-mpnet-base-v2**
- Melhor compreens√£o contextual
- 768 dimens√µes

#### ‚ö° Alta Performance (Muitos Documentos)
‚Üí **all-MiniLM-L6-v2**
- Mais r√°pido
- Menor consumo de mem√≥ria

#### üí∞ Dom√≠nio Espec√≠fico (Legal, M√©dico)
‚Üí **Fine-tune** custom model
- Treinar em dados do dom√≠nio
- M√°xima qualidade

## üîÆ Pr√≥ximos Passos

1. **Experimente**: Teste diferentes modelos
2. **Me√ßa**: Compare qualidade das respostas
3. **Otimize**: Use GPU se dispon√≠vel
4. **Documente**: Anote qual modelo funciona melhor

## üìö Recursos

- [Sentence Transformers](https://www.sbert.net/)
- [Hugging Face Models](https://huggingface.co/models?library=sentence-transformers)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

---

**D√∫vidas?** Veja o [FAQ](../FAQ.md) ou abra uma [issue](https://github.com/patrickmcruz/rag-demo/issues).
