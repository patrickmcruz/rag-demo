# AceleraÃ§Ã£o por GPU

Este guia explica como habilitar e otimizar a aceleraÃ§Ã£o por GPU no RAG Demo.

## ğŸ“Š VisÃ£o Geral

A aplicaÃ§Ã£o suporta aceleraÃ§Ã£o por GPU para componentes de embeddings, proporcionando melhorias significativas de performance:

- **Embeddings (sentence-transformers)**: 3-5x mais rÃ¡pido com GPU
- **LLM (Ollama)**: Utiliza GPU automaticamente se disponÃ­vel
- **ChromaDB**: NÃ£o utiliza GPU (apenas armazenamento)

## ğŸ”§ Requisitos

### Hardware
- **GPU NVIDIA** com suporte a CUDA:
  - GeForce GTX 1060 (6GB) ou superior
  - RTX sÃ©rie 20xx/30xx/40xx (recomendado)
  - Quadro/Tesla para workstations
- **VRAM**: MÃ­nimo 4GB, recomendado 6GB+

### Software
- **CUDA Toolkit** 11.8 ou superior
- **Drivers NVIDIA** atualizados (versÃ£o 520+ para CUDA 11.8)
- **Python** 3.10-3.12

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Verificar Compatibilidade

Verifique se sua GPU Ã© compatÃ­vel:

```bash
# Windows
nvidia-smi

# Deve mostrar informaÃ§Ãµes da GPU e versÃ£o do driver
```

### 2. Instalar CUDA Toolkit (se necessÃ¡rio)

Baixe e instale do site oficial da NVIDIA:
- https://developer.nvidia.com/cuda-downloads

**Ou use o instalador do conda:**

```bash
conda install -c conda-forge cudatoolkit=11.8 cudnn=8.6
```

### 3. Instalar PyTorch com Suporte CUDA

```bash
# Ativar ambiente virtual
.venv\Scripts\activate

# Instalar PyTorch com CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Para CUDA 12.1 (GPUs mais recentes)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 4. Verificar InstalaÃ§Ã£o

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA disponÃ­vel: {torch.cuda.is_available()}'); print(f'VersÃ£o CUDA: {torch.version.cuda}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

**SaÃ­da esperada:**
```
PyTorch: 2.x.x+cu118
CUDA disponÃ­vel: True
VersÃ£o CUDA: 11.8
GPU: NVIDIA GeForce RTX 3060
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Habilitar GPU na AplicaÃ§Ã£o

Edite o arquivo `.env`:

```bash
# GPU Configuration
USE_GPU=true
GPU_DEVICE=0  # ID da GPU (0, 1, 2... para mÃºltiplas GPUs)
```

### MÃºltiplas GPUs

Se vocÃª tem mÃºltiplas GPUs, especifique qual usar:

```bash
# Listar GPUs disponÃ­veis
python -c "import torch; print(f'GPUs disponÃ­veis: {torch.cuda.device_count()}'); [print(f'  {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"

# Configurar GPU especÃ­fica no .env
GPU_DEVICE=1  # Usar segunda GPU
```

## ğŸš€ Uso

### IngestÃ£o de Documentos com GPU

```bash
# Com GPU habilitada no .env
python main.py ingest

# Logs devem mostrar:
# INFO - Using device: cuda:0
# INFO - GPU available: NVIDIA GeForce RTX 3060, using cuda:0
```

### Consultas com GPU

```bash
python main.py query -q "Sua pergunta aqui"

# Modo interativo
python main.py query --interactive
```

## ğŸ“ˆ Benchmark de Performance

### Embeddings (all-MiniLM-L6-v2)

| OperaÃ§Ã£o | CPU (Intel i7) | GPU (RTX 3060) | Speedup |
|----------|----------------|----------------|---------|
| IngestÃ£o de 100 PDFs | ~180s | ~45s | 4.0x |
| Embedding de 1000 chunks | ~25s | ~6s | 4.2x |
| Query (top-k=3) | ~0.8s | ~0.2s | 4.0x |

### Ollama (LLM)

Ollama detecta GPU automaticamente, nÃ£o requer configuraÃ§Ã£o adicional:

```bash
# Verificar se Ollama estÃ¡ usando GPU
ollama run llama3 "test"

# Monitorar uso da GPU
nvidia-smi -l 1
```

## ğŸ” Monitoramento

### Uso de GPU em Tempo Real

```bash
# Monitor contÃ­nuo (atualiza a cada 1 segundo)
nvidia-smi -l 1

# InformaÃ§Ãµes detalhadas
nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,utilization.memory,memory.total,memory.used,memory.free --format=csv
```

### Durante IngestÃ£o/Query

```powershell
# Terminal 1: Executar aplicaÃ§Ã£o
python main.py ingest

# Terminal 2: Monitorar GPU
nvidia-smi -l 1
```

## ğŸ› Troubleshooting

### "CUDA not available" (PyTorch instalado mas GPU nÃ£o detectada)

**Causa:** VersÃ£o do PyTorch incompatÃ­vel com versÃ£o do CUDA

**SoluÃ§Ã£o:**
```bash
# Verificar versÃ£o CUDA do sistema
nvcc --version

# Reinstalar PyTorch com versÃ£o correta
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### "RuntimeError: CUDA out of memory"

**Causa:** VRAM insuficiente para batch de embeddings

**SoluÃ§Ã£o:**
1. Reduzir batch size (sentence-transformers usa batch automÃ¡tico)
2. Processar menos documentos por vez
3. Usar modelo de embedding menor:

```bash
# .env
EMBEDDING_MODEL=paraphrase-MiniLM-L3-v2  # Menor que all-MiniLM-L6-v2
```

### GPU nÃ£o acelera significativamente

**Causa:** Overhead de transferÃªncia CPUâ†”GPU para batches pequenos

**SoluÃ§Ã£o:**
- GPU Ã© mais eficiente para ingestÃ£o em lote (muitos documentos)
- Para queries isoladas, a diferenÃ§a Ã© menor
- Considere usar CPU se processar poucos documentos

### Drivers NVIDIA desatualizados

**Causa:** Driver incompatÃ­vel com CUDA Toolkit

**SoluÃ§Ã£o:**
```bash
# Verificar versÃ£o do driver
nvidia-smi

# Baixar driver atualizado:
# https://www.nvidia.com/Download/index.aspx
```

## ğŸ’¡ OtimizaÃ§Ãµes AvanÃ§adas

### 1. Precision Reduzida (FP16)

Para GPUs com Tensor Cores (RTX sÃ©rie):

```python
# Modificar src/chain.py e src/ingest.py
embedding = HuggingFaceEmbeddings(
    model_name=self.embedding_model,
    model_kwargs={
        'device': self.device,
        'torch_dtype': torch.float16  # FP16 para velocidade
    },
    encode_kwargs={'normalize_embeddings': True}
)
```

### 2. Batch Size Otimizado

```python
# src/ingest.py - adicionar ao criar embeddings
encode_kwargs={
    'normalize_embeddings': True,
    'batch_size': 128  # Aumentar para GPUs potentes
}
```

### 3. MÃºltiplas GPUs (Data Parallel)

Para datasets muito grandes com mÃºltiplas GPUs:

```python
# Modificar src/chain.py
import torch.nn as nn

if torch.cuda.device_count() > 1:
    embedding_model = nn.DataParallel(embedding_model)
```

## ğŸ”— Recursos Adicionais

- [PyTorch CUDA SemÃ¢ntica](https://pytorch.org/docs/stable/notes/cuda.html)
- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
- [Sentence Transformers Performance](https://www.sbert.net/docs/usage/computing_sentence_embeddings.html#performance)
- [Ollama GPU Support](https://github.com/ollama/ollama/blob/main/docs/gpu.md)

## ğŸ“Š ComparaÃ§Ã£o: CPU vs GPU

### Quando usar GPU:
âœ… IngestÃ£o de grandes volumes de documentos (100+ PDFs)  
âœ… ReindexaÃ§Ã£o frequente do vectorstore  
âœ… MÃºltiplas queries simultÃ¢neas  
âœ… Modelos de embedding grandes (>100M parÃ¢metros)  

### Quando CPU Ã© suficiente:
âœ… Queries ocasionais em vectorstore jÃ¡ construÃ­do  
âœ… Poucos documentos (<50)  
âœ… Modelos pequenos (all-MiniLM-L6-v2)  
âœ… Ambiente de desenvolvimento/testes  

## ğŸ¯ Checklist PrÃ©-Deploy

Antes de usar GPU em produÃ§Ã£o:

- [ ] GPU tem VRAM suficiente (6GB+)
- [ ] Drivers NVIDIA atualizados
- [ ] CUDA Toolkit instalado corretamente
- [ ] PyTorch detecta CUDA (`torch.cuda.is_available()` = True)
- [ ] VariÃ¡veis `USE_GPU=true` e `GPU_DEVICE=X` configuradas
- [ ] Testes de benchmark realizados
- [ ] Monitoramento de GPU implementado
- [ ] Fallback para CPU configurado

---

**ğŸ’¬ DÃºvidas?** Abra uma issue em https://github.com/patrickmcruz/rag-demo/issues
