# Guia de Modelos LLM

Escolha, configure e otimize modelos de linguagem (LLM) no RAG Demo.

## ğŸ“‹ O que sÃ£o LLMs?

**LLM (Large Language Model)** Ã© o modelo que gera as respostas finais baseado no contexto recuperado.

No RAG Demo, usamos **Ollama** para rodar modelos localmente.

## ğŸ¯ Modelo PadrÃ£o: Llama 3

### EspecificaÃ§Ãµes

```yaml
Nome: llama3 (Meta AI)
Tamanho: ~4.7GB (quantizado 4-bit)
ParÃ¢metros: 8B (8 bilhÃµes)
Context Window: 8192 tokens
Velocidade: ~20-30 tokens/s (CPU), ~100+ tokens/s (GPU)
LicenÃ§a: Llama 3 Community License
```

### Por que Llama 3?

**âœ… Vantagens:**
- Excelente qualidade (comparÃ¡vel a GPT-3.5)
- Suporte multilÃ­ngue (bom portuguÃªs)
- Gratuito e open source
- Roda localmente (privacidade total)
- Quantizado 4-bit (economiza memÃ³ria)

**âš ï¸ LimitaÃ§Ãµes:**
- Requer hardware moderado (4GB+ RAM)
- Mais lento que APIs cloud
- Context window menor que GPT-4

## ğŸš€ Modelos DisponÃ­veis no Ollama

### ComparaÃ§Ã£o Completa

| Modelo | Tamanho | ParÃ¢metros | Velocidade | Qualidade | PortuguÃªs |
|--------|---------|------------|------------|-----------|-----------|
| **llama3** | 4.7GB | 8B | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| llama3:70b | 40GB | 70B | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| mistral | 4.1GB | 7B | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| phi3 | 2.2GB | 3.8B | â­â­â­â­â­ | â­â­â­ | â­â­â­ |
| gemma2 | 5.4GB | 9B | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| qwen2.5 | 4.4GB | 7B | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |

### 1. Llama 3 (Recomendado)

**Melhor para**: Uso geral, boa qualidade

```bash
ollama pull llama3
```

**CaracterÃ­sticas**:
- Balanceio perfeito qualidade/velocidade
- Excelente compreensÃ£o contextual
- Bom em portuguÃªs
- 8K context window

**Uso no RAG Demo**:
```bash
python main.py --model llama3 query -q "pergunta"
```

### 2. Llama 3:70b (MÃ¡xima Qualidade)

**Melhor para**: Tarefas complexas, hardware potente

```bash
ollama pull llama3:70b
```

**CaracterÃ­sticas**:
- Melhor qualidade disponÃ­vel
- 70 bilhÃµes de parÃ¢metros
- Requer GPU potente
- Mais lento

**Requisitos**: 48GB+ RAM ou GPU com 40GB+ VRAM

### 3. Mistral (Mais RÃ¡pido)

**Melhor para**: Respostas rÃ¡pidas, baixa latÃªncia

```bash
ollama pull mistral
```

**CaracterÃ­sticas**:
- Mais rÃ¡pido que Llama 3
- Boa qualidade
- 4.1GB
- InglÃªs > PortuguÃªs

### 4. Phi3 (Mais Leve)

**Melhor para**: Hardware limitado, testes rÃ¡pidos

```bash
ollama pull phi3
```

**CaracterÃ­sticas**:
- Apenas 2.2GB
- Muito rÃ¡pido
- Qualidade razoÃ¡vel
- Bom para prototipagem

### 5. Gemma2 (Google)

**Melhor para**: Alta qualidade, Google-trained

```bash
ollama pull gemma2
```

**CaracterÃ­sticas**:
- Desenvolvido pelo Google
- Excelente qualidade
- Bom multilÃ­ngue
- 9B parÃ¢metros

### 6. Qwen2.5 (Alibaba)

**Melhor para**: MultilÃ­ngue, Ãsia-focado

```bash
ollama pull qwen2.5
```

**CaracterÃ­sticas**:
- Forte em mÃºltiplos idiomas
- Desenvolvido pela Alibaba
- 7B parÃ¢metros
- Boa qualidade geral

## âš™ï¸ Trocar Modelo

### Via Linha de Comando

```bash
# OpÃ§Ã£o 1: Argumento global
python main.py --model mistral query -q "pergunta"

# OpÃ§Ã£o 2: Modo interativo
python main.py --model phi3 query --interactive
```

### Via Arquivo .env

```env
# .env
OLLAMA_MODEL=mistral
```

Depois execute normalmente:
```bash
python main.py query -q "pergunta"
```

### Via CÃ³digo

```python
# src/chain.py - linha ~75
llm = OllamaLLM(
    model="mistral",  # Alterar aqui
    temperature=self.temperature,
)
```

## ğŸ›ï¸ ParÃ¢metros do LLM

### Temperature

Controla aleatoriedade das respostas:

```python
# Factual (recomendado para RAG)
temperature=0.0  # Respostas determinÃ­sticas

# Balanceado
temperature=0.3  # Pouca criatividade

# Criativo
temperature=0.7  # Mais variado

# Muito criativo
temperature=1.0  # MÃ¡xima aleatoriedade
```

**No RAG Demo**:
```bash
python main.py query -q "pergunta" --temperature 0.7
```

### Top-K (Retrieval)

NÃºmero de documentos recuperados:

```bash
# Mais focado
--top-k 1  # Apenas 1 documento

# PadrÃ£o (recomendado)
--top-k 3  # 3 documentos

# Mais contexto
--top-k 5  # 5 documentos

# Muito contexto (pode ter ruÃ­do)
--top-k 10
```

### Top-P e Top-K (Sampling)

Controla vocabulÃ¡rio na geraÃ§Ã£o:

```python
# Em Ollama (futuro)
llm = OllamaLLM(
    model="llama3",
    temperature=0.7,
    top_k=40,  # Top 40 tokens mais provÃ¡veis
    top_p=0.9,  # Nucleus sampling
)
```

## ğŸ“Š Benchmarks

### Tempo de Resposta (CPU Intel i7)

| Modelo | Tempo MÃ©dio | Tokens/segundo |
|--------|-------------|----------------|
| phi3 | 0.5s | 50 |
| mistral | 1.2s | 25 |
| llama3 | 2.0s | 15 |
| gemma2 | 2.5s | 12 |
| llama3:70b | 15s | 2 |

### Qualidade (Benchmark MMLU)

| Modelo | Score | Rank |
|--------|-------|------|
| llama3:70b | 79.2% | Top 1% |
| gemma2 | 71.3% | Top 5% |
| llama3 | 68.4% | Top 10% |
| qwen2.5 | 65.5% | Top 15% |
| mistral | 60.1% | Top 20% |
| phi3 | 68.8% | Top 10% |

## ğŸ”§ OtimizaÃ§Ã£o

### 1. Usar GPU

**AutomÃ¡tico**: Ollama usa GPU se disponÃ­vel

**Verificar**:
```bash
ollama run llama3 --verbose
# Mostra: "Using GPU: NVIDIA RTX 3080"
```

**Ganho**: 5-10x mais rÃ¡pido

### 2. QuantizaÃ§Ã£o

Modelos jÃ¡ vÃªm quantizados (4-bit), mas vocÃª pode escolher:

```bash
# Mais rÃ¡pido, menor qualidade
ollama pull llama3:q4_0

# Balanceado (padrÃ£o)
ollama pull llama3

# Melhor qualidade, mais lento
ollama pull llama3:q8_0

# Sem quantizaÃ§Ã£o (muito lento)
ollama pull llama3:fp16
```

### 3. Context Window

Modelos tÃªm limites de context:

| Modelo | Context Window |
|--------|----------------|
| llama3 | 8192 tokens |
| mistral | 8192 tokens |
| gemma2 | 8192 tokens |
| phi3 | 4096 tokens |

**âš ï¸ Cuidado**: Muito contexto = mais lento

### 4. Streaming

Respostas em tempo real (futuro):

```python
for chunk in llm.stream("pergunta"):
    print(chunk, end="", flush=True)
```

## ğŸ¯ Escolhendo o Modelo

### Por Caso de Uso

#### ğŸ“„ Documentos Corporativos
â†’ **llama3** (balanceado)

#### âš¡ ProtÃ³tipo RÃ¡pido
â†’ **phi3** (leve e rÃ¡pido)

#### ğŸ“ AnÃ¡lise Profunda
â†’ **llama3:70b** (mÃ¡xima qualidade)

#### ğŸŒ MÃºltiplos Idiomas
â†’ **qwen2.5** (multilÃ­ngue)

#### ğŸ’° Hardware Limitado
â†’ **phi3** (apenas 2.2GB)

### Por Hardware

#### ğŸ’» CPU (8GB RAM)
â†’ **phi3** ou **mistral**

#### ğŸ’» CPU (16GB+ RAM)
â†’ **llama3** ou **gemma2**

#### ğŸ® GPU (8GB VRAM)
â†’ **llama3** ou **mistral**

#### ğŸ® GPU (16GB+ VRAM)
â†’ **gemma2** ou **qwen2.5**

#### ğŸš€ GPU (40GB+ VRAM)
â†’ **llama3:70b**

## ğŸ“š Modelos Especializados

### CÃ³digo (ProgramaÃ§Ã£o)

```bash
ollama pull codellama
ollama pull deepseek-coder
```

### MatemÃ¡tica

```bash
ollama pull wizardmath
ollama pull llemma
```

### Medicina

```bash
ollama pull meditron
ollama pull biomedlm
```

## ğŸ†˜ Troubleshooting

### "Model not found"

```bash
# Listar modelos disponÃ­veis
ollama list

# Baixar modelo
ollama pull llama3
```

### Muito Lento

```bash
# Usar modelo menor
ollama pull phi3

# Ou verificar GPU
nvidia-smi  # Linux
```

### Respostas Ruins

```bash
# Aumentar temperatura
--temperature 0.7

# Ou usar modelo melhor
ollama pull gemma2
```

## ğŸ”® PrÃ³ximos Passos

1. **Teste**: Experimente diferentes modelos
2. **Compare**: Veja qual funciona melhor para seu caso
3. **Otimize**: Use GPU se disponÃ­vel
4. **Documente**: Anote configuraÃ§Ãµes que funcionam

## ğŸ“š Recursos

- [Ollama Models](https://ollama.ai/library)
- [Llama 3 Paper](https://ai.meta.com/llama/)
- [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

---

**DÃºvidas?** Consulte o [FAQ](../FAQ.md) ou abra uma [issue](https://github.com/patrickmcruz/rag-demo/issues).
