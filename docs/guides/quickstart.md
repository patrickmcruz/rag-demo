# Guia de InÃ­cio RÃ¡pido

Configure e execute o **RAG Demo** em menos de 10 minutos.

## ğŸ“‹ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter:

- âœ… **Python 3.9+** instalado
- âœ… **Ollama** instalado e rodando
- âœ… **4GB+ RAM** disponÃ­vel
- âœ… **~2GB espaÃ§o em disco** livre

## ğŸš€ Passo 1: Instalar Ollama e Modelo

### Windows/Mac
1. Baixe o Ollama: [https://ollama.ai](https://ollama.ai)
2. Instale e execute
3. Baixe o modelo Llama 3:

```bash
ollama pull llama3
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3
```

### Verificar InstalaÃ§Ã£o
```bash
ollama list
# Deve mostrar: llama3

ollama run llama3 "Hello"
# Deve responder com uma mensagem
```

## ğŸ“¦ Passo 2: Clonar e Configurar Projeto

```bash
# 1. Clonar repositÃ³rio
git clone https://github.com/patrickmcruz/rag-demo.git
cd rag-demo

# 2. Criar ambiente virtual
python -m venv .venv

# 3. Ativar ambiente virtual
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1

# Windows CMD:
.venv\Scripts\activate.bat

# Linux/Mac:
source .venv/bin/activate

# 4. Instalar dependÃªncias
pip install -r requirements.txt
```

**âš ï¸ Importante**: Sempre ative o ambiente virtual antes de executar comandos!

## ğŸ“„ Passo 3: Adicionar Documentos

Coloque seus documentos (PDF, TXT, MD) na pasta `data/`:

```bash
# Criar pasta data (se nÃ£o existir)
mkdir data

# Copiar seus documentos
cp seus_documentos.pdf data/

# Ou baixar exemplos
# curl -o data/exemplo.pdf https://exemplo.com/documento.pdf
```

**Formatos suportados**:
- âœ… PDF (`.pdf`)
- âœ… Texto (`.txt`)
- âœ… Markdown (`.md`)

## ğŸ” Passo 4: Indexar Documentos

Execute a ingestÃ£o para processar e indexar seus documentos:

```bash
python main.py ingest
```

**O que acontece**:
1. â³ Carrega documentos da pasta `data/`
2. âœ‚ï¸ Divide em chunks de 500 caracteres
3. ğŸ§  Gera embeddings (baixa modelo ~90MB na primeira vez)
4. ğŸ’¾ Salva Ã­ndice em `./vectorstore/`

**Tempo estimado**: ~10 segundos para 50 pÃ¡ginas

**SaÃ­da esperada**:
```
2025-11-26 10:00:00 - INFO - Loading pdf files from ./data
2025-11-26 10:00:02 - INFO - Loaded 52 pdf documents
2025-11-26 10:00:02 - INFO - Created 435 chunks
2025-11-26 10:00:10 - INFO - Successfully ingested 52 documents
```

## ğŸ’¬ Passo 5: Fazer Perguntas

### OpÃ§Ã£o A: Query Ãšnica

```bash
python main.py query -q "Qual o conteÃºdo do documento?"
```

### OpÃ§Ã£o B: Modo Interativo

```bash
python main.py query --interactive
```

Depois digite suas perguntas:
```
> Qual o assunto principal?
Resposta: O documento trata sobre...

> Quais os pontos importantes?
Resposta: Os principais pontos sÃ£o...

> exit
```

## ğŸ¯ Exemplos PrÃ¡ticos

### Exemplo 1: Consultar Edital

```bash
# Adicionar edital
cp edital_concurso.pdf data/

# Indexar
python main.py ingest

# Consultar
python main.py query -q "Quais sÃ£o os cargos disponÃ­veis?"
python main.py query -q "Qual o salÃ¡rio?"
python main.py query -q "Como se inscrever?"
```

### Exemplo 2: Analisar Contratos

```bash
# Adicionar contratos
cp contratos/*.pdf data/

# Indexar
python main.py ingest

# Consultar
python main.py query -q "Quais as clÃ¡usulas de rescisÃ£o?"
python main.py query -q "Qual o prazo de vigÃªncia?"
```

### Exemplo 3: Pesquisar DocumentaÃ§Ã£o TÃ©cnica

```bash
# Adicionar docs
cp documentacao/*.md data/

# Indexar
python main.py ingest

# Consultar
python main.py query -q "Como configurar o sistema?"
python main.py query --interactive
```

## âš™ï¸ OpÃ§Ãµes AvanÃ§adas

### Customizar IngestÃ£o

```bash
# Apenas PDFs
python main.py ingest --file-types pdf

# Chunks menores
python main.py ingest --chunk-size 300 --chunk-overlap 30

# MÃºltiplos tipos
python main.py ingest --file-types pdf,txt,md
```

### Customizar Query

```bash
# Recuperar mais documentos
python main.py query -q "pergunta" --top-k 5

# Respostas mais criativas
python main.py query -q "pergunta" --temperature 0.7

# Usar modelo diferente
python main.py --model mistral query -q "pergunta"
```

### Ver InformaÃ§Ãµes do Sistema

```bash
python main.py info
```

## ğŸ”§ ConfiguraÃ§Ãµes (.env)

Crie/edite o arquivo `.env` para personalizar:

```env
# Modelo LLM
OLLAMA_MODEL=llama3

# Embedding
EMBEDDING_MODEL=all-MiniLM-L6-v2

# DiretÃ³rios
DATA_DIR=./data
VECTORSTORE_DIR=./vectorstore

# RAG Config
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K_DOCUMENTS=3
TEMPERATURE=0.0

# Desabilitar telemetria
ANONYMIZED_TELEMETRY=False
```

## â“ Problemas Comuns

### "Vector store not found"
**SoluÃ§Ã£o**: Execute `python main.py ingest` primeiro

### "Ollama call failed with status code 404"
**SoluÃ§Ã£o**: Instale o modelo: `ollama pull llama3`

### "ModuleNotFoundError"
**SoluÃ§Ã£o**: Ative o ambiente virtual: `.\.venv\Scripts\Activate.ps1`

### "Microsoft Visual C++ 14.0 required" (Windows)
**SoluÃ§Ã£o**: Instale [Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**Mais soluÃ§Ãµes**: Veja [Troubleshooting Guide](troubleshooting.md)

## ğŸ“ PrÃ³ximos Passos

Agora que o sistema estÃ¡ rodando:

1. ğŸ“– Leia o [Guia de Modelos](models.md) para trocar de LLM
2. ğŸ§  Veja o [Guia de Embeddings](embeddings.md) para otimizar
3. ğŸ“š Consulte o [FAQ](../FAQ.md) para dÃºvidas comuns
4. ğŸ—ï¸ Entenda a [Arquitetura](../ARCHITECTURE.md) do sistema

## ğŸ’¡ Dicas

- âœ… Sempre ative o ambiente virtual
- âœ… Re-indexe apÃ³s adicionar novos documentos
- âœ… Use `--interactive` para explorar documentos
- âœ… Comece com poucos documentos para testar
- âœ… Ajuste `temperature` conforme necessidade

## ğŸ†˜ Precisa de Ajuda?

- ğŸ“– [FAQ Completo](../FAQ.md)
- ğŸ› [Troubleshooting](troubleshooting.md)
- ğŸ’¬ [GitHub Issues](https://github.com/patrickmcruz/rag-demo/issues)
- ğŸ“§ Email: patrickmcruz@gmail.com

---

**Pronto!** ğŸ‰ Seu sistema RAG estÃ¡ funcionando. Boas consultas!
