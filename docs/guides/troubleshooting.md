# Guia de Troubleshooting

Solu√ß√µes para problemas comuns no RAG Demo.

## üìã √çndice
- [Instala√ß√£o](#instala√ß√£o)
- [Ingest√£o](#ingest√£o)
- [Queries](#queries)
- [Performance](#performance)
- [Ollama](#ollama)

---

## üîß Instala√ß√£o

### 1. Erro: "Microsoft Visual C++ 14.0 or greater is required"

**Problema**: Ao instalar depend√™ncias no Windows, falta compilador C++.

**Solu√ß√£o A** (Recomendada):
```bash
# Instalar Build Tools
# Download: https://visualstudio.microsoft.com/visual-cpp-build-tools/
# Durante instala√ß√£o, selecione "Desktop development with C++"
```

**Solu√ß√£o B**:
```bash
# Ativar Developer Mode
# Configura√ß√µes ‚Üí Para desenvolvedores ‚Üí Modo de Desenvolvedor
```

**Solu√ß√£o C** (Workaround):
```bash
# Remover chroma-hnswlib do requirements.txt (n√£o √© obrigat√≥rio)
```

### 2. Erro: "`np.float_` was removed in NumPy 2.0"

**Problema**: Incompatibilidade entre NumPy 2.0+ e sentence-transformers.

**Solu√ß√£o**:
```bash
pip install "numpy==1.26.4" --force-reinstall
```

### 3. Erro: "ModuleNotFoundError: No module named 'langchain_community'"

**Problema**: Ambiente virtual n√£o est√° ativado ou depend√™ncias n√£o foram instaladas.

**Solu√ß√£o**:
```bash
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1

# Linux/Mac:
source .venv/bin/activate

# Reinstalar depend√™ncias
pip install -r requirements.txt
```

### 4. SSL Certificate Errors (Ambientes Corporativos)

**Problema**: Erros de certificado ao baixar modelos.

**Solu√ß√£o**:
```bash
# Temporariamente (n√£o recomendado em produ√ß√£o)
set CURL_CA_BUNDLE=
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
```

---

## üìÑ Ingest√£o

### 5. Erro: "No documents found in ./data"

**Problema**: Pasta `data/` vazia ou n√£o cont√©m arquivos suportados.

**Solu√ß√£o**:
```bash
# Verificar conte√∫do
ls data/

# Adicionar documentos
cp seus_documentos.pdf data/

# Verificar tipos de arquivo suportados
python main.py ingest --file-types pdf,txt,md
```

### 6. Erro ao processar PDFs

**Problema**: PDF corrompido ou protegido.

**Solu√ß√£o**:
```bash
# Verificar integridade do PDF
# Tentar abrir em leitor de PDF

# Remover prote√ß√£o (se permitido)
# qpdf --decrypt input.pdf output.pdf
```

### 7. Ingest√£o Lenta

**Problema**: Processamento demora muito.

**Solu√ß√£o**:
```bash
# Reduzir chunk_size
python main.py ingest --chunk-size 300

# Processar menos arquivos por vez
python main.py ingest --file-types pdf  # Apenas PDFs

# Usar GPU (se dispon√≠vel)
# Autom√°tico se GPU detectada
```

---

## üí¨ Queries

### 8. Erro: "Vector store not found at: ./vectorstore"

**Problema**: Tentando fazer query antes de indexar documentos.

**Solu√ß√£o**:
```bash
# Primeiro indexe os documentos
python main.py ingest

# Depois fa√ßa queries
python main.py query -q "sua pergunta"
```

### 9. Erro: "Ollama call failed with status code 404"

**Problema**: Modelo Ollama n√£o est√° instalado.

**Solu√ß√£o**:
```bash
# Verificar modelos instalados
ollama list

# Instalar modelo necess√°rio
ollama pull llama3

# Ou usar modelo j√° instalado
python main.py --model mistral query -q "pergunta"
```

### 10. Respostas de Baixa Qualidade

**Problema**: Respostas imprecisas ou irrelevantes.

**Solu√ß√£o A** - Aumentar contexto:
```bash
python main.py query -q "pergunta" --top-k 5
```

**Solu√ß√£o B** - Trocar modelo:
```bash
ollama pull gemma2
python main.py --model gemma2 query -q "pergunta"
```

**Solu√ß√£o C** - Ajustar temperatura:
```bash
python main.py query -q "pergunta" --temperature 0.7
```

**Solu√ß√£o D** - Re-indexar com chunks diferentes:
```bash
python main.py ingest --chunk-size 700 --chunk-overlap 100
```

### 11. Respostas Muito Lentas

**Problema**: Tempo de resposta > 10s.

**Solu√ß√£o**:
```bash
# Usar modelo mais r√°pido
ollama pull phi3
python main.py --model phi3 query -q "pergunta"

# Reduzir top-k
python main.py query -q "pergunta" --top-k 1

# Verificar se GPU est√° sendo usada
ollama ps
```

---

## üöÄ Performance

### 12. Alto Uso de Mem√≥ria

**Problema**: Sistema consome muita RAM.

**Solu√ß√£o**:
```bash
# Usar modelo menor
ollama pull phi3  # Apenas 2.2GB

# Limpar vectorstore antigo
rm -rf vectorstore/
python main.py ingest

# Reduzir batch size (futuro)
```

### 13. Ollama N√£o Responde

**Problema**: Ollama travado ou n√£o iniciado.

**Solu√ß√£o**:
```bash
# Verificar se est√° rodando
ollama list

# Reiniciar Ollama
# Windows: Fechar e reabrir aplica√ß√£o
# Linux: sudo systemctl restart ollama

# Verificar porta
curl http://localhost:11434/api/tags
```

### 14. Erro de Porta (11434) em Uso

**Problema**: Outra aplica√ß√£o usando porta do Ollama.

**Solu√ß√£o**:
```bash
# Windows: Verificar porta
netstat -ano | findstr :11434

# Linux/Mac:
lsof -i :11434

# Mudar porta Ollama (n√£o recomendado)
# Ou fechar aplica√ß√£o conflitante
```

---

## ü§ñ Ollama

### 15. "Model not found" ao fazer pull

**Problema**: Modelo n√£o existe ou nome incorreto.

**Solu√ß√£o**:
```bash
# Listar modelos dispon√≠veis
ollama list

# Buscar modelos online
# https://ollama.ai/library

# Usar nome correto
ollama pull llama3  # N√£o "lama3" ou "llama-3"
```

### 16. Download Interrompido

**Problema**: Download do modelo falhou.

**Solu√ß√£o**:
```bash
# Tentar novamente (retoma automaticamente)
ollama pull llama3

# Verificar espa√ßo em disco
df -h  # Linux/Mac
wmic logicaldisk get size,freespace  # Windows

# Verificar conex√£o internet
ping ollama.ai
```

### 17. Modelo Corrompido

**Problema**: Modelo baixado est√° corrompido.

**Solu√ß√£o**:
```bash
# Remover modelo
ollama rm llama3

# Baixar novamente
ollama pull llama3

# Verificar integridade
ollama run llama3 "test"
```

---

## üìä Diagn√≥stico

### Coletar Informa√ß√µes de Debug

```bash
# Vers√µes
python --version
pip --version
ollama --version

# Modelos instalados
ollama list

# Status do sistema
python main.py info

# Logs detalhados
export LOG_LEVEL=DEBUG  # Linux/Mac
set LOG_LEVEL=DEBUG  # Windows
python main.py ingest
```

### Testar Componentes

```python
# Testar embeddings
python -c "
from langchain_huggingface import HuggingFaceEmbeddings
emb = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
print('Embeddings OK')
"

# Testar Ollama
python -c "
from langchain_ollama import OllamaLLM
llm = OllamaLLM(model='llama3')
print(llm.invoke('test'))
"

# Testar ChromaDB
python -c "
from langchain_chroma import Chroma
print('ChromaDB OK')
"
```

---

## üÜò Ainda com Problemas?

### Relatar Problema

Ao abrir uma issue, inclua:

1. **Sistema Operacional**: Windows/Linux/Mac
2. **Vers√£o Python**: `python --version`
3. **Vers√£o Ollama**: `ollama --version`
4. **Comando executado**: Ex: `python main.py ingest`
5. **Erro completo**: Copiar mensagem de erro inteira
6. **Logs**: Executar com `LOG_LEVEL=DEBUG`

### Recursos

- üìñ [FAQ Completo](../FAQ.md)
- üèóÔ∏è [Arquitetura](../ARCHITECTURE.md)
- üí¨ [GitHub Issues](https://github.com/patrickmcruz/rag-demo/issues)
- üìß Email: patrickmcruz@gmail.com

---

**Dica**: A maioria dos problemas se resolve com:
1. ‚úÖ Ativar ambiente virtual
2. ‚úÖ Reinstalar depend√™ncias
3. ‚úÖ Verificar Ollama est√° rodando
4. ‚úÖ Re-indexar documentos
